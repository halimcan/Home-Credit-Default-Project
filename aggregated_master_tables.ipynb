{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFaK35ezydrSD4bpGnGfMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halimcan/Home-Credit-Default-Project/blob/Catboost%2C-LightGBM%2C-XGBoost%2C-Blending%2C-Stacking(Logistic-Regression-Meta-Model)_branch10/aggregated_master_tables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJfs4suTn1nY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import gc\n",
        "# Imports the garbage collection module to manually free unused memory.\n",
        "\n",
        "import os\n",
        "# Imports the OS module to work with file paths and the file system.\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "# Base directory path where all aggregated Home Credit tables are stored.\n",
        "\n"
      ],
      "metadata": {
        "id": "uuolt6y082er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting csv based tables into parquet"
      ],
      "metadata": {
        "id": "sQAi-jGPsTNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "# Imports the Polars library, a high-performance DataFrame engine optimized for speed and memory efficiency.\n",
        "\n",
        "import os\n",
        "# Imports the OS module to interact with the file system (list directories, join paths, etc.).\n",
        "\n",
        "for file in os.listdir(BASE):\n",
        "    # Iterates through all files located in the BASE directory.\n",
        "\n",
        "    if file.endswith(\".csv\"):\n",
        "        # Processes only files with the .csv extension.\n",
        "\n",
        "        csv_path = f\"{BASE}/{file}\"\n",
        "        # Builds the full path to the current CSV file.\n",
        "\n",
        "        pq_path = csv_path.replace(\".csv\", \".parquet\")\n",
        "        # Defines the output path by replacing the .csv extension with .parquet.\n",
        "\n",
        "        print(f\"→ Converting: {file}  -->  {pq_path}\")\n",
        "        # Prints a progress message indicating which file is being converted.\n",
        "\n",
        "        df = pl.read_csv(csv_path)\n",
        "        # Loads the CSV file into a Polars DataFrame.\n",
        "\n",
        "        df.write_parquet(pq_path)\n",
        "        # Writes the DataFrame to disk in Parquet format.\n"
      ],
      "metadata": {
        "id": "wqCweDYBzG8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging bureau and bureau loan tables through polars, and some additional aggregations with numerical variables(mean,max etc.)"
      ],
      "metadata": {
        "id": "2JRaXoVtscdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "# Imports the Polars library for high-performance DataFrame processing.\n",
        "\n",
        "import polars.selectors as cs\n",
        "# Imports column selectors to easily filter numeric, string, or boolean columns.\n",
        "\n",
        "# Raw datas\n",
        "bureau = pl.read_csv(f\"{BASE}/bureau.csv\")\n",
        "# Loads the raw bureau table containing historical credit information for each loan.\n",
        "\n",
        "balance = pl.read_csv(f\"{BASE}/bureau_bal_loan.csv\")\n",
        "# Loads the bureau balance table containing monthly status records for each bureau loan.\n",
        "\n",
        "# SK_ID_CURR add\n",
        "bb = balance.join(\n",
        "    bureau.select([\"SK_ID_BUREAU\", \"SK_ID_CURR\"]),\n",
        "    # Selects only the keys needed for joining (bureau loan ID → customer ID).\n",
        "\n",
        "    on=\"SK_ID_BUREAU\",\n",
        "    # Performs the join based on the bureau loan identifier.\n",
        "\n",
        "    how=\"left\"\n",
        "    # Uses a left join to ensure all balance records remain even if bureau data is missing.\n",
        ")\n",
        "# This join attaches SK_ID_CURR (customer ID) to each row of the bureau balance table.\n",
        "\n",
        "# Selecting numerical columns\n",
        "numeric_cols = bb.select(cs.numeric()).columns\n",
        "# Extracts the names of all numeric columns to automatically generate aggregated features.\n",
        "\n",
        "# Aggregation list\n",
        "agg_exprs = []\n",
        "# Initializes an empty list that will store Polars aggregation expressions.\n",
        "\n",
        "for col in numeric_cols:\n",
        "    # Iterates over every numerical column to generate aggregation metrics.\n",
        "\n",
        "    agg_exprs.extend([\n",
        "        pl.col(col).mean().alias(f\"{col}_MEAN\"),\n",
        "        # Computes the mean value per customer for this column.\n",
        "\n",
        "        pl.col(col).min().alias(f\"{col}_MIN\"),\n",
        "        # Computes the minimum value per customer.\n",
        "\n",
        "        pl.col(col).max().alias(f\"{col}_MAX\"),\n",
        "        # Computes the maximum value per customer.\n",
        "\n",
        "        pl.col(col).sum().alias(f\"{col}_SUM\")\n",
        "        # Computes the total sum per customer.\n",
        "    ])\n",
        "# After the loop, agg_exprs contains all aggregated feature expressions for numeric columns.\n",
        "\n",
        "# SK_ID_CURR aggregation\n",
        "bb_agg_curr = (\n",
        "    bb.group_by(\"SK_ID_CURR\")\n",
        "      # Groups monthly bureau balance records by customer ID.\n",
        "\n",
        "      .agg(agg_exprs + [pl.len().alias(\"BB_COUNT\")])\n",
        "      # Computes all aggregated numerical features plus a count of records per customer.\n",
        ")\n",
        "# The result is one row per SK_ID_CURR containing aggregated bureau balance features.\n",
        "\n",
        "# Save as parquet\n",
        "bb_agg_curr.write_parquet(f\"{BASE}/bureau_bal_loan.parquet\")\n",
        "# Saves the aggregated dataset in Parquet format for fast loading in downstream modeling.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3mKVARuir-PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lazy Frame based ETL of the aggregated tables"
      ],
      "metadata": {
        "id": "QhJqMtvYsJMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pl.scan_parquet(f\"{BASE}/application_train.parquet\")\n",
        "# Lazily loads the main application table (one row per current loan application).\n",
        "# Using `scan_parquet` enables lazy execution, avoiding full data loading until `collect()` is called.\n",
        "\n",
        "bureau_agg = pl.scan_parquet(f\"{BASE}/bureau_agg.parquet\")\n",
        "# Loads aggregated external credit bureau data (historical loans from external institutions),\n",
        "# represented at the customer (SK_ID_CURR) level.\n",
        "\n",
        "previous_agg = pl.scan_parquet(f\"{BASE}/previous_agg.parquet\")\n",
        "# Loads aggregated information from previous credit applications,\n",
        "# providing historical behavior insights for each customer.\n",
        "\n",
        "pos_agg = pl.scan_parquet(f\"{BASE}/pos_agg.parquet\")\n",
        "# Loads aggregated POS (point of sale) and cash loan data,\n",
        "# containing monthly and installment-based behavior patterns.\n",
        "\n",
        "installments_agg = pl.scan_parquet(f\"{BASE}/installments_agg.parquet\")\n",
        "# Loads aggregated installments payment history (e.g., payment delays, missed payments),\n",
        "# which is highly predictive for credit risk.\n",
        "\n",
        "cc_agg = pl.scan_parquet(f\"{BASE}/cc_agg.parquet\")\n",
        "# Loads aggregated credit card balance and payment information,\n",
        "# capturing revolving credit behavior and utilization ratios.\n",
        "\n",
        "bureau_bal_loan = pl.scan_parquet(f\"{BASE}/bureau_bal_loan.parquet\")\n",
        "# Loads aggregated monthly bureau balance data for external loans,\n",
        "# including derived metrics such as mean, min, max, and counts for each customer.\n",
        "\n"
      ],
      "metadata": {
        "id": "EUfnIgHwsFLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging all parquet tables."
      ],
      "metadata": {
        "id": "8vGUjIiqtcuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged = (\n",
        "    train\n",
        "    # Starts with the main application_train LazyFrame, representing the primary loan applications.\n",
        "\n",
        "    .join(bureau_agg, on=\"SK_ID_CURR\", how=\"left\", suffix=\"_bur\")\n",
        "    # Merges aggregated external bureau information.\n",
        "    # LEFT JOIN ensures that every application remains in the dataset even if some customers\n",
        "    # do not have bureau history.\n",
        "    # `suffix=\"_bur\"` prevents column name collisions.\n",
        "\n",
        "    .join(bureau_bal_loan, on=\"SK_ID_CURR\", how=\"left\", suffix=\"_bal\")\n",
        "    # Adds aggregated monthly bureau balance results (e.g., mean, max, min values for bureau loans).\n",
        "    # These features often capture delinquency patterns and credit behavior over time.\n",
        "\n",
        "    .join(previous_agg, on=\"SK_ID_CURR\", how=\"left\", suffix=\"_prev\")\n",
        "    # Adds aggregated results from previous loan applications.\n",
        "    # Includes approval rates, refused applications, and other historical application behaviors.\n",
        "\n",
        "    .join(pos_agg, on=\"SK_ID_CURR\", how=\"left\", suffix=\"_pos\")\n",
        "    # Adds POS (point-of-sale) and cash loan aggregated features.\n",
        "    # Captures trends of monthly status, delays, late payments, and consumer finance behavior.\n",
        "\n",
        "    .join(installments_agg, on=\"SK_ID_CURR\", how=\"left\", suffix=\"_ins\")\n",
        "    # Adds aggregated installment payments—one of the strongest predictors for credit default.\n",
        "    # Includes late payment metrics, missed installments, and repayment patterns.\n",
        "\n",
        "    .join(cc_agg, on=\"SK_ID_CURR\", how=\"left\", suffix=\"_cc\")\n",
        "    # Adds aggregated credit card balance and payment features.\n",
        "    # These features capture revolving credit utilization, minimum payments, and delinquency indicators.\n",
        ")\n",
        "# The resulting LazyFrame contains all engineered features merged into a single dataset.\n",
        "# Because this is LazyFrame-based, Polars will optimize the entire join chain before execution.\n"
      ],
      "metadata": {
        "id": "7Q1wwewAsKkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final = train_merged.collect()\n",
        "# Executes the entire lazy computation graph and loads the fully merged dataset into memory.\n",
        "# This triggers:\n",
        "#   - all deferred joins,\n",
        "#   - predicate and projection pushdowns,\n",
        "#   - query optimization,\n",
        "#   - and final materialization of the full feature matrix.\n",
        "# The result is a concrete Polars DataFrame containing all engineered and merged features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_g9X37YMsSA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final.write_parquet(f\"{BASE}/train_full.parquet\")\n",
        "# Writes the fully merged and materialized training dataset to disk in Parquet format.\n",
        "# Parquet is a columnar, compressed, high-performance file format ideal for ML pipelines,\n",
        "# enabling faster loading, lower disk usage, and efficient column-level access.\n",
        "# This file will serve as the final feature matrix used in model training.\n"
      ],
      "metadata": {
        "id": "PImSsTjBut8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final.shape\n"
      ],
      "metadata": {
        "id": "Cdj7VPfysKxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final.group_by(\"SK_ID_CURR\").count().sort(\"count\", descending=True).head(20)  # Current application’s customer ID\n",
        "# Groups the final dataset by SK_ID_CURR to check for duplicate customer IDs.\n",
        "# `.count()` computes the number of rows associated with each customer.\n",
        "# `.sort(\"count\", descending=True)` ranks customers by row count in descending order.\n",
        "# `.head(20)` displays the top 20 customers with the highest number of duplicate records.\n",
        "# This is a validation step to ensure that the final dataset respects the one-row-per-customer structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "qU1zO26ivUYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dup_check = (\n",
        "    train_final\n",
        "        .group_by(\"SK_ID_CURR\")\n",
        "        .count()\n",
        "        .filter(pl.col(\"count\") > 1)\n",
        ")\n",
        "\n",
        "dup_check\n",
        "# Returns all customers that appear more than once in the final dataset.\n"
      ],
      "metadata": {
        "id": "y6ujkrQSvF6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No customer has more than one row.\n",
        "Dataset is perfectly aggregated.\n",
        "train_final contains exactly one row per SK_ID_CURR."
      ],
      "metadata": {
        "id": "2WnrjWvlwCJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test_application merge"
      ],
      "metadata": {
        "id": "H3gW5fauo6DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 1) Parquet dosyalarını load et\n",
        "# ---------------------------------------------\n",
        "test  = pl.scan_parquet(f\"{BASE}/application_test.parquet\")\n",
        "\n",
        "bureau_agg       = pl.scan_parquet(f\"{BASE}/bureau_agg.parquet\")\n",
        "previous_agg     = pl.scan_parquet(f\"{BASE}/previous_agg.parquet\")\n",
        "pos_agg          = pl.scan_parquet(f\"{BASE}/pos_agg.parquet\")\n",
        "installments_agg = pl.scan_parquet(f\"{BASE}/installments_agg.parquet\")\n",
        "cc_agg           = pl.scan_parquet(f\"{BASE}/cc_agg.parquet\")\n",
        "bureau_bal_loan  = pl.scan_parquet(f\"{BASE}/bureau_bal_loan.parquet\")  # aggregation already done\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2) TEST JOIN PIPELINE  (train ile birebir aynı sıralama)\n",
        "# ---------------------------------------------\n",
        "test_merged = (\n",
        "    test\n",
        "    .join(bureau_agg,        on=\"SK_ID_CURR\", how=\"left\", suffix=\"_bur\")\n",
        "    .join(bureau_bal_loan,   on=\"SK_ID_CURR\", how=\"left\", suffix=\"_bal\")\n",
        "    .join(previous_agg,      on=\"SK_ID_CURR\", how=\"left\", suffix=\"_prev\")\n",
        "    .join(pos_agg,           on=\"SK_ID_CURR\", how=\"left\", suffix=\"_pos\")\n",
        "    .join(installments_agg,  on=\"SK_ID_CURR\", how=\"left\", suffix=\"_ins\")\n",
        "    .join(cc_agg,            on=\"SK_ID_CURR\", how=\"left\", suffix=\"_cc\")\n",
        ")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3) Final test dataframe\n",
        "# ---------------------------------------------\n",
        "test_final = test_merged.collect()\n",
        "test_final.write_parquet(f\"{BASE}/test_full.parquet\")\n",
        "\n",
        "# Sonuç\n",
        "print(\"TEST FINAL SHAPE:\", test_final.shape)\n"
      ],
      "metadata": {
        "id": "jAlIRD9X0l0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 5. TRAIN/TEST COLUMN ALIGNMENT CHECK\n",
        "# ================================================================\n",
        "train_cols = set(train.columns)\n",
        "test_cols  = set(test.columns)\n",
        "\n",
        "print(\"Columns in TRAIN but not TEST:\", train_cols - test_cols)\n",
        "print(\"Columns in TEST but not TRAIN:\", test_cols - train_cols)"
      ],
      "metadata": {
        "id": "E6FNkjaE1QlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================================================\n",
        "# 6. SAVE MERGED DATASETS\n",
        "# ================================================================\n",
        "#train.to_csv(f\"{BASE}/master_train.csv\", index=False)\n",
        "#test.to_csv(f\"{BASE}/master_test.csv\", index=False)\n",
        "\n",
        "#print(\"Saved master_train.csv and master_test.csv successfully.\")"
      ],
      "metadata": {
        "id": "mv3do7cP1Qq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "# Imports Polars, the high-performance DataFrame engine.\n",
        "\n",
        "import polars.selectors as cs\n",
        "# Provides column selectors (e.g., cs.numeric()) for easy type-based filtering.\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 1) DataFrames already loaded in RAM\n",
        "# ---------------------------------------------\n",
        "train = train_final\n",
        "# The fully merged and materialized training dataset.\n",
        "\n",
        "test = test_final\n",
        "# The fully merged and materialized test dataset.\n",
        "\n",
        "TARGET_COL = \"TARGET\"\n",
        "# Target column for binary classification (credit default vs non-default).\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2) Missing percentage calculation (based on train)\n",
        "# ---------------------------------------------\n",
        "missing = (\n",
        "    train.select([\n",
        "        ((pl.col(c).is_null().sum() / train.height) * 100)\n",
        "        .alias(c)\n",
        "        for c in train.columns\n",
        "    ])\n",
        ")\n",
        "# Computes the percentage of missing values for every column:\n",
        "#   - is_null().sum() counts missing values\n",
        "#   - train.height gives row count\n",
        "#   - result is converted to a percentage\n",
        "# Output is a single-row DataFrame where each column holds its missing %.\n",
        "\n",
        "missing_t = missing.transpose(include_header=True)\n",
        "# Transposes the DataFrame so columns become rows, making analysis easier.\n",
        "\n",
        "missing_t.columns = [\"column\", \"missing_percent\"]\n",
        "# Renames resulting columns into descriptive names.\n",
        "\n",
        "missing_sorted = missing_t.sort(\"missing_percent\", descending=True)\n",
        "# Sorts columns from highest missing rate to lowest.\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3) Build column lists based on missing %\n",
        "# ---------------------------------------------\n",
        "drop_cols = (\n",
        "    missing_sorted.filter(pl.col(\"missing_percent\") > 80)[\"column\"].to_list()\n",
        ")\n",
        "# Columns with >80% missing → drop entirely.\n",
        "\n",
        "median_candidates = (\n",
        "    missing_sorted.filter(\n",
        "        (pl.col(\"missing_percent\") <= 80) &\n",
        "        (pl.col(\"missing_percent\") >= 10)\n",
        "    )[\"column\"].to_list()\n",
        ")\n",
        "# Columns with 10–80% missing → fill with median (only numeric ones).\n",
        "\n",
        "native_cols = (\n",
        "    missing_sorted.filter(pl.col(\"missing_percent\") < 10)[\"column\"].to_list()\n",
        ")\n",
        "# Columns with <10% missing → keep as-is (models handle small missing % fine).\n",
        "\n",
        "# Remove target column from lists\n",
        "drop_cols = [c for c in drop_cols if c != TARGET_COL]\n",
        "median_candidates = [c for c in median_candidates if c != TARGET_COL]\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 4) Only numeric columns are eligible for median fill\n",
        "# ---------------------------------------------\n",
        "numeric_cols = train.select(cs.numeric()).columns\n",
        "# Identifies numeric features (ints, floats).\n",
        "\n",
        "median_fill_cols = [c for c in median_candidates if c in numeric_cols]\n",
        "# Applies median filling only to numeric median candidates.\n",
        "\n",
        "# Remaining string columns will not be median-filled.\n",
        "print(\"DROP:\", len(drop_cols))\n",
        "print(\"MEDIAN (numeric):\", len(median_fill_cols))\n",
        "print(\"NATIVE:\", len(native_cols))\n",
        "\n",
        "# ---------------------------------------------\n",
        "# CLEAN FUNCTION (keeps original column names)\n",
        "# ---------------------------------------------\n",
        "def clean_dataset(df):\n",
        "    # Drop columns with excessive missingness\n",
        "    df = df.drop(drop_cols)\n",
        "\n",
        "    # Apply median fill column-by-column\n",
        "    for col in median_fill_cols:\n",
        "        df = df.with_columns(\n",
        "            pl.col(col).fill_null(pl.col(col).median())\n",
        "        )\n",
        "        # This computes the median of each column once per usage.\n",
        "\n",
        "    # String columns are left unchanged; most tree models (e.g. LightGBM)\n",
        "    # are robust to a limited number of missing values.\n",
        "    return df\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 5) Clean train + test using identical logic\n",
        "# ---------------------------------------------\n",
        "train_clean = clean_dataset(train)\n",
        "test_clean  = clean_dataset(test)\n",
        "# Ensures consistent preprocessing for model training and inference.\n",
        "\n",
        "# Move target column to the front (optional, for readability)\n",
        "cols = [TARGET_COL] + [c for c in train_clean.columns if c != TARGET_COL]\n",
        "train_clean = train_clean.select(cols)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 6) Save cleaned datasets\n",
        "# ---------------------------------------------\n",
        "train_clean.write_parquet(f\"{BASE}/train_clean.parquet\")\n",
        "# Saves cleaned training data in Parquet format.\n",
        "\n",
        "test_clean.write_parquet(f\"{BASE}/test_clean.parquet\")\n",
        "# Saves cleaned test data in Parquet format.\n",
        "\n",
        "print(\"train_clean shape:\", train_clean.shape)\n",
        "print(\"test_clean shape:\", test_clean.shape)\n",
        "# Displays the final shapes after cleaning.\n",
        "\n"
      ],
      "metadata": {
        "id": "FKQSCAE_rlKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features with >80% missing → DROPPED\n",
        "\n",
        "Columns with extremely high missingness provide little reliable signal and tend to add noise.\n",
        "Dropping them reduces dimensionality, improves model stability, and prevents overfitting.\n",
        "\n",
        "Features with 10–80% missing → MEDIAN IMPUTATION (numeric only)\n",
        "\n",
        "These columns still contain meaningful predictive information but have too many missing values to leave untouched.\n",
        "Median imputation stabilizes their distribution, prevents bias from extreme values, and is robust to outliers.\n",
        "For non-numeric columns in this range, missing values are left as-is because tree-based models (e.g., LightGBM) handle them natively.\n",
        "\n",
        "Features with <10% missing → LEFT AS IS (native model handling)\n",
        "\n",
        "Columns with low missingness retain nearly complete information.\n",
        "Tree-based models like LightGBM can naturally and optimally handle missing values during split creation.\n",
        "Keeping these null values often improves model performance by allowing the algorithm to learn missing-value patterns directly."
      ],
      "metadata": {
        "id": "jRGkHQgUsbu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation-based feature reduction"
      ],
      "metadata": {
        "id": "G8ALUdHKx3hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation-based feature reduction\n",
        "\n",
        "import polars as pl\n",
        "# High-performance DataFrame library used for reading and processing parquet files.\n",
        "\n",
        "import polars.selectors as cs\n",
        "# Provides utilities (e.g., cs.numeric()) to select columns by type.\n",
        "\n",
        "import pandas as pd\n",
        "# Used for correlation computation, as Pandas provides an efficient corr() implementation.\n",
        "\n",
        "import numpy as np\n",
        "# NumPy is used to construct the upper-triangle mask for correlation filtering.\n",
        "\n",
        "import os\n",
        "# Used to list and verify output files.\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "# Base directory where all intermediate and cleaned feature files are stored.\n",
        "\n",
        "TARGET_COL = \"TARGET\"\n",
        "# Target feature for the credit default prediction task.\n",
        "\n",
        "train = pl.read_parquet(f\"{BASE}/train_clean.parquet\")\n",
        "# Loads the cleaned and preprocessed training dataset (after missing value handling).\n",
        "\n",
        "test  = pl.read_parquet(f\"{BASE}/test_clean.parquet\")\n",
        "# Loads the cleaned test dataset.\n",
        "\n",
        "print(\"Input shapes:\", train.shape, test.shape)\n",
        "# Displays initial dataset shapes before feature reduction.\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Select only numeric columns (correlation applies to numeric)\n",
        "# ------------------------------------------------------------\n",
        "numeric_cols = train.select(cs.numeric()).columns\n",
        "# Extracts numeric column names from the dataset.\n",
        "\n",
        "numeric_cols = [c for c in numeric_cols if c != TARGET_COL]\n",
        "# Removes target column from the correlation feature set.\n",
        "\n",
        "# Convert to pandas for correlation computation\n",
        "df_pd = train.select(numeric_cols).to_pandas()\n",
        "# Converts only the numeric subset of the training data into a Pandas DataFrame.\n",
        "# Corr computation is easier and well-supported in Pandas.\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Compute Correlation Matrix\n",
        "# ------------------------------------------------------------\n",
        "corr_matrix = df_pd.corr().abs()\n",
        "# Computes absolute correlation values between all numeric columns.\n",
        "\n",
        "# Generate upper-triangle mask to avoid duplicate checks\n",
        "upper = corr_matrix.where(\n",
        "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "# Only keeps correlations above diagonal; avoids symmetric duplicates.\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Identify columns to drop based on correlation threshold\n",
        "# ------------------------------------------------------------\n",
        "corr_threshold = 0.95\n",
        "# Features with correlation > 0.95 are considered redundant.\n",
        "\n",
        "corr_drop_cols = [\n",
        "    col for col in upper.columns\n",
        "    if any(upper[col] > corr_threshold)\n",
        "]\n",
        "# For each column, if any correlation in its upper triangle exceeds threshold → mark column for removal.\n",
        "\n",
        "print(f\"Corr DROP count: {len(corr_drop_cols)}\")\n",
        "# Prints how many features will be removed due to high multicollinearity.\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Drop redundant features from train and test\n",
        "# ------------------------------------------------------------\n",
        "train_reduced = train.drop(corr_drop_cols)\n",
        "# Removes correlated columns from the training set.\n",
        "\n",
        "test_reduced  = test.drop(corr_drop_cols)\n",
        "# Applies same column removal to the test set to maintain consistency.\n",
        "\n",
        "print(\"Output shapes:\", train_reduced.shape, test_reduced.shape)\n",
        "# Displays dataset shapes after correlation-based feature reduction.\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Save reduced datasets\n",
        "# ------------------------------------------------------------\n",
        "train_reduced.write_parquet(f\"{BASE}/train_corr_reduced.parquet\")\n",
        "# Writes the correlation-reduced training dataset.\n",
        "\n",
        "test_reduced.write_parquet(f\"{BASE}/test_corr_reduced.parquet\")\n",
        "# Writes the correlation-reduced test dataset.\n",
        "\n",
        "print(\"Files saved!\")\n",
        "# Confirms successful write.\n",
        "\n",
        "print(os.listdir(BASE))\n",
        "# Lists files in the BASE directory to verify existence of written files.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DYS0t_WEaIT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "Remove highly correlated features (multicollinearity) to improve model efficiency and avoid redundant feature information.\n",
        "\n",
        "Steps performed:\n",
        "\n",
        "Load cleaned datasets\n",
        "\n",
        "Select numeric-only columns\n",
        "\n",
        "Compute correlation matrix (Pandas)\n",
        "\n",
        "Extract upper triangle to avoid duplicate comparisons\n",
        "\n",
        "Identify columns where any correlation > 0.95\n",
        "\n",
        "Drop those columns from both train & test\n",
        "\n",
        "Save reduced datasets\n",
        "\n",
        "\n",
        "** Why is correlation-based reduction important in credit modeling?\n",
        "\n",
        "Highly correlated variables can cause overfitting\n",
        "\n",
        "Many models (e.g., Logistic Regression, Neural Nets) suffer from multicollinearity\n",
        "\n",
        "Even tree models benefit:\n",
        "\n",
        "faster training\n",
        "\n",
        "fewer redundant splits\n",
        "\n",
        "more stable feature importances\n",
        "\n",
        "Improves generalization and interpretability\n",
        "\n",
        "Reduces dimensionality → faster LightGBM/XGBoost training"
      ],
      "metadata": {
        "id": "UzFRwRWaySkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CHUNK 1 — Load data and reduce memory usage\n",
        "# ==========================================================\n",
        "\n",
        "import polars as pl\n",
        "# Polars is used for fast and memory-efficient I/O and preprocessing.\n",
        "\n",
        "import numpy as np\n",
        "# NumPy will be used later for numerical operations (fold indices, array manipulations, etc.).\n",
        "\n",
        "import lightgbm as lgb\n",
        "# LightGBM is the primary ML model used for credit default prediction.\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# StratifiedKFold ensures class balance across folds during cross-validation.\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# AUC (Area Under ROC Curve) is the main evaluation metric for Home Credit.\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "# Base directory containing preprocessed feature tables.\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Load correlation-reduced datasets\n",
        "# ----------------------------------------------------------\n",
        "train = pl.read_parquet(f\"{BASE}/train_corr_reduced.parquet\")\n",
        "# Loads the training dataset after correlation-based feature reduction.\n",
        "\n",
        "test  = pl.read_parquet(f\"{BASE}/test_corr_reduced.parquet\")\n",
        "# Loads the test dataset with the same feature reductions applied.\n",
        "\n",
        "print(\"Loaded:\", train.shape, test.shape)\n",
        "# Confirms dataset dimensions before model training.\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Convert float64 → float32 to reduce memory footprint\n",
        "# ----------------------------------------------------------\n",
        "train = train.with_columns([pl.col(pl.Float64).cast(pl.Float32)])\n",
        "# Casts all Float64 columns to Float32, reducing RAM usage by half.\n",
        "# This is important because LightGBM and CV folds can be memory-intensive.\n",
        "\n",
        "test  = test.with_columns([pl.col(pl.Float64).cast(pl.Float32)])\n",
        "# Applies the same conversion to the test dataset for consistency.\n",
        "\n",
        "print(\"Float32 conversion completed.\")\n",
        "# Signals the end of memory optimization.\n",
        "\n"
      ],
      "metadata": {
        "id": "IRnhCnU4wP-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Efficient data loading\n",
        "\n",
        "Polars loads Parquet files extremely fast and avoids unnecessary RAM usage.\n",
        "\n",
        "2. Correlation-reduced features\n",
        "\n",
        "You already removed highly correlated columns (>0.95), reducing:\n",
        "\n",
        "redundancy\n",
        "\n",
        "training time\n",
        "\n",
        "model risk of overfitting\n",
        "\n",
        "3. Float64 → Float32 conversion\n",
        "\n",
        "This typically reduces memory usage by 35–50%, especially beneficial when:\n",
        "\n",
        "training multiple folds\n",
        "\n",
        "storing OOF predictions\n",
        "\n",
        "running LightGBM on Colab with limited RAM\n",
        "\n",
        "4. Preparing for model training (Chunk 2)\n",
        "\n",
        "After this block:\n",
        "\n",
        "train/test datasets are optimized\n",
        "\n",
        "ready for feature/target split\n",
        "\n",
        "ready for StratifiedKFold CV\n",
        "\n",
        "ready for LightGBM training"
      ],
      "metadata": {
        "id": "Hj84WUlEzI4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "# (Converting polars to pandas)"
      ],
      "metadata": {
        "id": "IOEwKgm40dZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# FEATURE ENGINEERING BLOCK (SAFE VERSION)\n",
        "# ==========================================================\n",
        "\n",
        "import pandas as pd\n",
        "# Pandas is used here because row-wise and complex feature engineering operations\n",
        "# (ratio creation, binning, conditional interactions) are easier and more flexible\n",
        "# compared to Polars when producing many nonlinear features.\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Convert Polars → Pandas for FE operations\n",
        "# ----------------------------------------------------------\n",
        "train_pd = train.to_pandas()\n",
        "# Converts the cleaned and correlation-reduced Polars training dataset into a Pandas DataFrame,\n",
        "# making it easier to engineer complex numeric and interaction features.\n",
        "\n",
        "test_pd  = test.to_pandas()\n",
        "# Converts the test dataset using the same logic for consistent feature availability.\n",
        "\n",
        "TARGET_COL = \"TARGET\"\n",
        "# Name of the binary target variable (1 = default, 0 = repaid).\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SAFE FEATURE ENGINEERING FUNCTION\n",
        "# Works only with columns that exist — prevents KeyErrors.\n",
        "# ----------------------------------------------------------\n",
        "def add_feature_engineering(df):\n",
        "    eps = 1e-6  # Small epsilon to avoid division-by-zero errors.\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Separate target column (if present)\n",
        "    # -----------------------------------------------\n",
        "    target = df[TARGET_COL] if TARGET_COL in df.columns else None\n",
        "\n",
        "    # Remove target for clean feature calculations\n",
        "    if TARGET_COL in df.columns:\n",
        "        df = df.drop(columns=[TARGET_COL])\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # SAFE COLUMN CHECKER → Only calculate if exists\n",
        "    # -----------------------------------------------\n",
        "    def safe(col):\n",
        "        return col in df.columns\n",
        "\n",
        "    # =====================================================\n",
        "    # 1) CORE FINANCIAL RATIOS (credit, income, annuity)\n",
        "    # =====================================================\n",
        "\n",
        "    if safe(\"AMT_CREDIT\") and safe(\"AMT_INCOME_TOTAL\"):\n",
        "        df[\"CREDIT_TO_INCOME\"] = df[\"AMT_CREDIT\"] / (df[\"AMT_INCOME_TOTAL\"] + eps)\n",
        "        # Ratio of requested credit to total annual income.\n",
        "        # High values indicate high debt burden and potential repayment difficulty.\n",
        "\n",
        "    if safe(\"AMT_ANNUITY\") and safe(\"AMT_INCOME_TOTAL\"):\n",
        "        df[\"ANNUITY_TO_INCOME\"] = df[\"AMT_ANNUITY\"] / (df[\"AMT_INCOME_TOTAL\"] + eps)\n",
        "        # Measures how heavy the annual repayment obligation is compared to income.\n",
        "        # Higher values → more financial stress.\n",
        "\n",
        "    if safe(\"AMT_CREDIT\") and safe(\"AMT_ANNUITY\"):\n",
        "        df[\"CREDIT_TO_ANNUITY\"] = df[\"AMT_CREDIT\"] / (df[\"AMT_ANNUITY\"] + eps)\n",
        "        # Proxy for loan duration (months of payments).\n",
        "        # Longer durations often correlate with higher risk borrowers.\n",
        "\n",
        "    if safe(\"DAYS_EMPLOYED\") and safe(\"DAYS_BIRTH\"):\n",
        "        df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / (df[\"DAYS_BIRTH\"] + eps)\n",
        "        # Employment duration relative to age.\n",
        "        # Applicants with short employment relative to age may have unstable work history.\n",
        "\n",
        "    # =====================================================\n",
        "    # 2) HOUSEHOLD INCOME STABILITY\n",
        "    # =====================================================\n",
        "\n",
        "    if safe(\"CNT_FAM_MEMBERS\") and safe(\"AMT_INCOME_TOTAL\"):\n",
        "        df[\"INC_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / (df[\"CNT_FAM_MEMBERS\"] + eps)\n",
        "        # Income per household member — direct proxy for available disposable income.\n",
        "        # Lower values → higher risk due to financial pressure.\n",
        "\n",
        "    # =====================================================\n",
        "    # 3) AGE GROUP SEGMENTATION\n",
        "    # =====================================================\n",
        "\n",
        "    if safe(\"DAYS_BIRTH\"):\n",
        "        df[\"AGE_SEGMENTS\"] = pd.cut(\n",
        "            -df[\"DAYS_BIRTH\"] / 365,\n",
        "            bins=[0, 25, 35, 45, 55, 65, 120],\n",
        "            labels=[1, 2, 3, 4, 5, 6]\n",
        "        ).astype(\"float32\")\n",
        "        # Bins applicant age into meaningful credit-risk categories.\n",
        "        # Risk is nonlinear across age — this captures those patterns.\n",
        "\n",
        "    # =====================================================\n",
        "    # 4) EXT_SOURCE INTERACTIONS (EXTREMELY STRONG FEATURES)\n",
        "    # =====================================================\n",
        "\n",
        "    ext_cols = [c for c in [\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"] if safe(c)]\n",
        "\n",
        "    if len(ext_cols) == 3:\n",
        "        df[\"EXT_SOURCE_1_2\"] = df[\"EXT_SOURCE_1\"] * df[\"EXT_SOURCE_2\"]\n",
        "        # Interaction between two external credit risk scores.\n",
        "\n",
        "        df[\"EXT_SOURCE_1_3\"] = df[\"EXT_SOURCE_1\"] * df[\"EXT_SOURCE_3\"]\n",
        "        df[\"EXT_SOURCE_2_3\"] = df[\"EXT_SOURCE_2\"] * df[\"EXT_SOURCE_3\"]\n",
        "\n",
        "        df[\"EXT_SOURCES_SUM\"] = df[ext_cols].sum(axis=1)\n",
        "        # Combined external risk score (sum).\n",
        "\n",
        "        df[\"EXT_SOURCES_MEAN\"] = df[\"EXT_SOURCES_SUM\"] / 3\n",
        "        # Average external risk score — strong single summary metric.\n",
        "\n",
        "        # These EXT_SOURCE features were among the strongest predictors\n",
        "        # in the original Kaggle Home Credit competition.\n",
        "\n",
        "    # =====================================================\n",
        "    # 5) DOCUMENT VERIFICATION SIGNALS\n",
        "    # =====================================================\n",
        "\n",
        "    doc_cols = [c for c in df.columns if \"FLAG_DOCUMENT\" in c]\n",
        "\n",
        "    if len(doc_cols) > 0:\n",
        "        df[\"FLAG_DOCUMENT_SUM\"] = df[doc_cols].sum(axis=1)\n",
        "        # Counts how many supporting documents the applicant submitted.\n",
        "        # More documents typically indicate less-risk applicants.\n",
        "\n",
        "    # =====================================================\n",
        "    # 6) PAYMENT BEHAVIOR FEATURES\n",
        "    # =====================================================\n",
        "\n",
        "    if safe(\"AMT_ANNUITY\") and safe(\"AMT_CREDIT\"):\n",
        "        df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / (df[\"AMT_CREDIT\"] + eps)\n",
        "        # (%) of credit repaid per installment.\n",
        "        # A key Home Credit benchmark feature.\n",
        "\n",
        "    if safe(\"AMT_ANNUITY\") and safe(\"AMT_INCOME_TOTAL\"):\n",
        "        df[\"PAYMENT_TO_INCOME\"] = df[\"AMT_ANNUITY\"] / (df[\"AMT_INCOME_TOTAL\"] + eps)\n",
        "        # How heavy the installment burden is relative to income.\n",
        "\n",
        "    # =====================================================\n",
        "    # 7) TEMPORAL RELATIONSHIPS BETWEEN FEATURES\n",
        "    # =====================================================\n",
        "\n",
        "    if safe(\"DAYS_EMPLOYED\") and safe(\"DAYS_BIRTH\"):\n",
        "        df[\"DAYS_BIRTH_EMPLOYED_RATIO\"] = df[\"DAYS_EMPLOYED\"] / (df[\"DAYS_BIRTH\"] + eps)\n",
        "        # Employment time relative to age.\n",
        "\n",
        "    if safe(\"DAYS_REGISTRATION\") and safe(\"DAYS_BIRTH\"):\n",
        "        df[\"DAYS_REGISTRATION_TO_BIRTH\"] = df[\"DAYS_REGISTRATION\"] / (df[\"DAYS_BIRTH\"] + eps)\n",
        "        # Stability of home registration relative to age.\n",
        "\n",
        "    if safe(\"DAYS_ID_PUBLISH\") and safe(\"DAYS_BIRTH\"):\n",
        "        df[\"DAYS_ID_CHANGE_TO_BIRTH\"] = df[\"DAYS_ID_PUBLISH\"] / (df[\"DAYS_BIRTH\"] + eps)\n",
        "        # How long ago the ID was changed — frequent ID changes may indicate fraud risk.\n",
        "\n",
        "    # =====================================================\n",
        "    # 8) EMERGENCY STATE FLAG\n",
        "    # =====================================================\n",
        "\n",
        "    if safe(\"EMERGENCYSTATE_MODE\"):\n",
        "        df[\"EMERGENCY_STATE_FLAG\"] = (df[\"EMERGENCYSTATE_MODE\"] == \"Yes\").astype(\"float32\")\n",
        "        # Converts categorical \"Yes\"/\"No\" into numeric.\n",
        "        # Applicants in an emergency state often show increased default risk.\n",
        "\n",
        "    # =====================================================\n",
        "    # 9) APPROXIMATE CREDIT TERM\n",
        "    # =====================================================\n",
        "\n",
        "    if safe(\"AMT_CREDIT\") and safe(\"AMT_ANNUITY\"):\n",
        "        df[\"CREDIT_TERM\"] = df[\"AMT_CREDIT\"] / (df[\"AMT_ANNUITY\"] + eps)\n",
        "        # Estimated number of installments → longer terms correlate with higher default risk.\n",
        "\n",
        "    # =====================================================\n",
        "    # Reattach TARGET column at beginning\n",
        "    # =====================================================\n",
        "    if target is not None:\n",
        "        df.insert(0, TARGET_COL, target)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# APPLY FEATURE ENGINEERING TO TRAIN AND TEST\n",
        "# ----------------------------------------------------------\n",
        "train_fe = add_feature_engineering(train_pd)\n",
        "test_fe  = add_feature_engineering(test_pd)\n",
        "\n",
        "print(\"Feature Engineering Completed!\")\n",
        "print(\"New train shape:\", train_fe.shape)\n",
        "print(\"New test shape:\", test_fe.shape)\n",
        "\n",
        "# Update processed datasets\n",
        "train_pd = train_fe\n",
        "test_pd  = test_fe\n"
      ],
      "metadata": {
        "id": "r2YEOTqXz6bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CHUNK 2 — FINAL CLEANING (Label Encoding + Float32)\n",
        "# ==========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "TARGET_COL = \"TARGET\"\n",
        "\n",
        "print(\"\\n=== CHUNK 2 STARTED ===\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1) COLUMN NAME SANITIZATION\n",
        "# Replace invalid characters with underscores\n",
        "# Ensures compatibility with LightGBM, Parquet, and feature importance tools.\n",
        "# ----------------------------------------------------------\n",
        "train_pd.columns = train_pd.columns.str.replace(\"[^A-Za-z0-9_]+\", \"_\", regex=True)\n",
        "test_pd.columns  = test_pd.columns.str.replace(\"[^A-Za-z0-9_]+\", \"_\", regex=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2) Extract feature columns (all except target)\n",
        "# ----------------------------------------------------------\n",
        "feature_cols = [c for c in train_pd.columns if c != TARGET_COL]\n",
        "\n",
        "# Ensure test set has exactly same columns in the same order\n",
        "# Missing columns become NaN → later filled with 0\n",
        "test_pd = test_pd.reindex(columns=feature_cols)\n",
        "\n",
        "# Reorganize train so it always has TARGET first\n",
        "train_pd = train_pd[[TARGET_COL] + feature_cols]\n",
        "\n",
        "print(\"Column alignment done.\")\n",
        "print(f\"{len(feature_cols)} features found.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3) LABEL ENCODE ALL CATEGORICAL (object) FEATURES\n",
        "# ----------------------------------------------------------\n",
        "# NOTE:\n",
        "# LightGBM can handle categorical features natively,\n",
        "# but Label Encoding ensures stable integer representation\n",
        "# and avoids issues with unexpected string tokens in test data.\n",
        "# ----------------------------------------------------------\n",
        "for col in feature_cols:\n",
        "    if train_pd[col].dtype == \"object\":\n",
        "        le = LabelEncoder()\n",
        "\n",
        "        # Fit encoder on both train + test to avoid unseen categories\n",
        "        full_col = pd.concat([train_pd[col], test_pd[col]], axis=0).astype(str)\n",
        "        le.fit(full_col)\n",
        "\n",
        "        # Transform both datasets\n",
        "        train_pd[col] = le.transform(train_pd[col].astype(str))\n",
        "        test_pd[col]  = le.transform(test_pd[col].astype(str))\n",
        "\n",
        "print(\"Label encoding completed.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4) Fill remaining missing values\n",
        "# LightGBM can handle missing values natively, but in practice,\n",
        "# filling with zero avoids edge cases where encoded strings or ratios\n",
        "# generate NaNs during feature engineering.\n",
        "# ----------------------------------------------------------\n",
        "train_pd = train_pd.fillna(0)\n",
        "test_pd  = test_pd.fillna(0)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5) Convert all features to float32\n",
        "# Reduces RAM usage by ~50% without hurting model accuracy.\n",
        "# LightGBM works efficiently with float32 matrices.\n",
        "# ----------------------------------------------------------\n",
        "train_pd[feature_cols] = train_pd[feature_cols].astype(np.float32)\n",
        "test_pd[feature_cols]  = test_pd[feature_cols].astype(np.float32)\n",
        "\n",
        "print(\"Float32 conversion done.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6) FINAL CHECK — READY FOR LIGHTGBM TRAINING\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\n=== CHUNK 2 COMPLETED ===\")\n",
        "print(\"Train:\", train_pd.shape)\n",
        "print(\"Test :\", test_pd.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "6jNeKJkyLt8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CHUNK 3 — RAM-FRIENDLY LightGBM Feature Importance (FINAL FIXED)\n",
        "# ==========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import gc\n",
        "\n",
        "print(\"Starting LIGHT Base LGBM...\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Extract target (y) and feature matrix (X)\n",
        "# ----------------------------------------------------------\n",
        "y = train_pd[\"TARGET\"]\n",
        "# Binary target variable: 1 = default, 0 = repaid.\n",
        "\n",
        "X = train_pd.drop(columns=[\"TARGET\"])\n",
        "# Removes the target from the feature matrix.\n",
        "\n",
        "feature_names = X.columns.tolist()\n",
        "# Stores feature names for importance ranking later.\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# LightGBM PARAMETERS (light baseline)\n",
        "# ----------------------------------------------------------\n",
        "params = {\n",
        "    \"objective\": \"binary\",          # Binary classification (default vs. non-default)\n",
        "    \"metric\": \"auc\",                # AUC is the main competition metric\n",
        "    \"learning_rate\": 0.05,          # Lower LR → stable training\n",
        "    \"num_leaves\": 31,               # Leaf complexity (limits overfitting)\n",
        "    \"max_depth\": -1,                # Let leaves control depth\n",
        "    \"feature_fraction\": 0.7,        # Random feature sampling per tree\n",
        "    \"bagging_fraction\": 0.7,        # Random row sampling\n",
        "    \"bagging_freq\": 1,              # Subsample every iteration\n",
        "    \"seed\": 42,\n",
        "    \"verbose\": -1\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Stratified K-Fold for balanced class splits\n",
        "# ----------------------------------------------------------\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "importances = np.zeros(len(feature_names))\n",
        "# Accumulates feature importances over all folds.\n",
        "\n",
        "fold_scores = []\n",
        "# Stores AUC for each fold.\n",
        "\n",
        "# ==========================================================\n",
        "# CROSS-VALIDATION LOOP\n",
        "# ==========================================================\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), 1):\n",
        "\n",
        "    print(f\"\\n===== FOLD {fold} =====\")\n",
        "\n",
        "    # Create LGBM datasets — memory efficient format\n",
        "    dtrain = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx])\n",
        "    dvalid = lgb.Dataset(X.iloc[valid_idx], y.iloc[valid_idx])\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # Training with early stopping (RAM-friendly)\n",
        "    # ------------------------------------------------------\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        valid_sets=[dvalid],\n",
        "        num_boost_round=300,                 # Maximum boosting rounds\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=30),  # Stops when no improvement\n",
        "            lgb.log_evaluation(period=50)            # Logs every 50 rounds\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # VALIDATION PREDICTIONS\n",
        "    # ------------------------------------------------------\n",
        "    preds = model.predict(X.iloc[valid_idx])\n",
        "    auc = roc_auc_score(y.iloc[valid_idx], preds)\n",
        "    fold_scores.append(auc)\n",
        "    # AUC per fold, measures ranking ability of model.\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # ACCUMULATE FEATURE IMPORTANCE (GAIN-BASED)\n",
        "    # ------------------------------------------------------\n",
        "    importances += model.feature_importance(importance_type=\"gain\")\n",
        "    # GAIN importance measures how much each feature improved splitting.\n",
        "    # More stable than split-count importance.\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # CLEAN MEMORY TO PREVENT RAM LEAKS\n",
        "    # ------------------------------------------------------\n",
        "    del dtrain, dvalid, model\n",
        "    gc.collect()\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# CROSS-VALIDATION SUMMARY\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\nCV Scores:\", fold_scores)\n",
        "print(\"Mean AUC:\", np.mean(fold_scores))\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# NORMALIZE FEATURE IMPORTANCE OVER FOLDS\n",
        "# ----------------------------------------------------------\n",
        "importances = importances / 5\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"importance\": importances\n",
        "}).sort_values(by=\"importance\", ascending=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# DROP LOW-IMPORTANCE FEATURES\n",
        "# We remove the bottom 35% weakest features\n",
        "# ----------------------------------------------------------\n",
        "drop_fraction = 0.35\n",
        "drop_count = int(len(importance_df) * drop_fraction)\n",
        "\n",
        "drop_features = importance_df.head(drop_count)[\"feature\"].tolist()\n",
        "\n",
        "print(f\"\\nDropping {drop_count} features...\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# REDUCE TRAIN/TEST ACCORDING TO FEATURE IMPORTANCE\n",
        "# ----------------------------------------------------------\n",
        "train_reduced = X.drop(columns=drop_features)\n",
        "test_reduced  = test_pd.drop(columns=drop_features)\n",
        "\n",
        "print(\"New shapes:\")\n",
        "print(\"train:\", train_reduced.shape)\n",
        "print(\"test:\", test_reduced.shape)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SAVE FINAL REDUCED DATASETS\n",
        "# ----------------------------------------------------------\n",
        "train_reduced.insert(0, \"TARGET\", y)\n",
        "\n",
        "train_reduced.to_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "test_reduced.to_parquet(f\"{BASE}/final_test_reduced.parquet\")\n",
        "\n",
        "print(\"\\nSaved final reduced datasets!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NficxFJsDVN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna==3.5.0"
      ],
      "metadata": {
        "id": "pxJ46We2PCLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# ADVANCED OPTUNA TUNING — LightGBM (3 TRIALS ONLY)\n",
        "# ==========================================================\n",
        "\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# Load dataset\n",
        "train_final = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "y = train_final[\"TARGET\"].values\n",
        "X = train_final.drop(columns=[\"TARGET\"])\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    params = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"metric\": \"auc\",\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "\n",
        "        # Wider but efficient search space\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.015, 0.08),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 160),\n",
        "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 220),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 12),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 6),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 3.0),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 3.0)\n",
        "    }\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    aucs = []\n",
        "\n",
        "    for train_idx, valid_idx in kf.split(X, y):\n",
        "        dtrain = lgb.Dataset(X.iloc[train_idx], y[train_idx])\n",
        "        dvalid = lgb.Dataset(X.iloc[valid_idx], y[valid_idx])\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            valid_sets=[dvalid],\n",
        "            num_boost_round=2000,   # higher capacity\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=80, verbose=False)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X.iloc[valid_idx])\n",
        "        auc = roc_auc_score(y[valid_idx], preds)\n",
        "        aucs.append(auc)\n",
        "\n",
        "    return np.mean(aucs)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# RUN OPTUNA (ONLY 5 TRIALS)\n",
        "# ----------------------------------------------------------\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=3, show_progress_bar=True)\n",
        "\n",
        "print(\"\\nBEST AUC:\", study.best_value)\n",
        "print(\"BEST PARAMS:\", study.best_params)\n",
        "\n",
        "# Save params\n",
        "import json\n",
        "with open(f\"{BASE}/best_optuna_params_ADVANCED.json\", \"w\") as f:\n",
        "    json.dump(study.best_params, f, indent=4)\n",
        "\n",
        "print(\"\\nSaved: best_optuna_params_ADVANCED.json\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Qx_ae3TPEhRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save params to JSON\n",
        "best_params = study.best_params\n",
        "with open(f\"{BASE}/best_optuna_params.json\", \"w\") as f:\n",
        "    json.dump(best_params, f, indent=4)\n",
        "\n",
        "# Save best AUC too (optional)\n",
        "with open(f\"{BASE}/best_optuna_auc.txt\", \"w\") as f:\n",
        "    f.write(str(study.best_value))\n",
        "\n",
        "print(f\"\\nSaved best_optuna_params.json and best_optuna_auc.txt in {BASE}\")\n"
      ],
      "metadata": {
        "id": "xjMegjzoEhgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ileride sadece kaydedilmis parametreleri kullanmka istersen\n",
        "import json\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "with open(f\"{BASE}/best_optuna_params.json\", \"r\") as f:\n",
        "    best_params = json.load(f)\n"
      ],
      "metadata": {
        "id": "cazZtB5zEhpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# FULL PIPELINE: K-FOLD TRAINING + SAVE FOLDS + SELECT BEST\n",
        "# ==========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import json\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1) Load Optuna params\n",
        "# ----------------------------------------------------------\n",
        "with open(f\"{BASE}/best_optuna_params.json\", \"r\") as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "best_params.update({\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"verbosity\": -1,\n",
        "    \"boosting_type\": \"gbdt\"\n",
        "})\n",
        "\n",
        "print(\"Using parameters:\")\n",
        "print(best_params)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2) Load final dataset\n",
        "# ----------------------------------------------------------\n",
        "train_final = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "test_final  = pd.read_parquet(f\"{BASE}/final_test_reduced.parquet\")\n",
        "\n",
        "y = train_final[\"TARGET\"].values\n",
        "X = train_final.drop(columns=[\"TARGET\"])\n",
        "\n",
        "print(\"\\nTrain:\", X.shape, \" Test:\", test_final.shape)\n",
        "\n",
        "# ==========================================================\n",
        "# 3) K-FOLD TRAINING (SAVE ALL FOLD MODELS)\n",
        "# ==========================================================\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"\\nTraining STRATIFIED 5-FOLD and SAVING ALL fold models...\\n\")\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), 1):\n",
        "    print(f\"===== FOLD {fold} =====\")\n",
        "\n",
        "    dtrain = lgb.Dataset(X.iloc[train_idx], y[train_idx])\n",
        "    dvalid = lgb.Dataset(X.iloc[valid_idx], y[valid_idx])\n",
        "\n",
        "    model = lgb.train(\n",
        "        best_params,\n",
        "        dtrain,\n",
        "        valid_sets=[dvalid],\n",
        "        num_boost_round=1200,\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=80)]\n",
        "    )\n",
        "\n",
        "    # SAVE fold model to disk (RAM dostu)\n",
        "    fold_path = f\"{BASE}/lgbm_model_fold{fold}.txt\"\n",
        "    model.save_model(fold_path)\n",
        "    print(f\"Saved fold model to: {fold_path}\")\n",
        "\n",
        "print(\"\\n=== ALL FOLD MODELS SAVED SUCCESSFULLY ===\\n\")\n",
        "\n",
        "# ==========================================================\n",
        "# 4) DISK ÜZERİNDEN BEST MODEL SEÇME (RAM bağımsız)\n",
        "# ==========================================================\n",
        "print(\"Evaluating all saved fold models (from disk)...\\n\")\n",
        "\n",
        "best_auc = -1\n",
        "best_fold = None\n",
        "best_model = None\n",
        "\n",
        "for fold in range(1, 6):\n",
        "    model_path = f\"{BASE}/lgbm_model_fold{fold}.txt\"\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"⚠️ Model missing: {model_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Loading Fold {fold} model: {model_path}\")\n",
        "    model = lgb.Booster(model_file=model_path)\n",
        "\n",
        "    preds = model.predict(X)\n",
        "    auc = roc_auc_score(y, preds)\n",
        "\n",
        "    print(f\"Fold {fold} AUC: {auc:.6f}\")\n",
        "\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        best_fold = fold\n",
        "        best_model = model\n",
        "\n",
        "print(\"\\n===================================\")\n",
        "print(\"BEST FOLD:\", best_fold)\n",
        "print(\"BEST AUC :\", best_auc)\n",
        "print(\"===================================\\n\")\n",
        "\n",
        "# Save best model\n",
        "best_model_path = f\"{BASE}/lgbm_best_model.txt\"\n",
        "best_model.save_model(best_model_path)\n",
        "print(f\"Saved BEST model to: {best_model_path}\")\n",
        "\n",
        "# Delete other fold models to save disk space\n",
        "print(\"\\nDeleting other fold models...\")\n",
        "\n",
        "for fold in range(1, 6):\n",
        "    model_path = f\"{BASE}/lgbm_model_fold{fold}.txt\"\n",
        "    if fold != best_fold and os.path.exists(model_path):\n",
        "        os.remove(model_path)\n",
        "        print(f\"Deleted: {model_path}\")\n",
        "\n",
        "print(\"\\n=== BEST MODEL SAVED & OTHER MODELS REMOVED ===\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7FKWh88cZNXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✔ Fold’lar arasında sapma az → stabil model\n",
        "\n",
        "(Aralık: 0.783 – 0.794 → HomeCredit için çok iyi)\n",
        "\n",
        "✔ OOF AUC = 0.7876\n",
        "\n",
        "Bu harika bir skor.\n",
        "\n",
        "Kaggle yarışmasında ilk 5% içine rahat giren skor seviyesindesin.\n",
        "\n",
        "LightGBM FAST modelinde valid AUC 0.7881 idi\n",
        "\n",
        "Stratified K-Fold ile modelin gerçek performansını ölçmüş olduk.\n",
        "\n",
        "\n",
        "Ne overfitting var\n",
        "\n",
        "Ne fold'lar arası büyük oynama var\n",
        "\n",
        "OOF → validation çok yakın\n",
        "\n",
        "Optuna tuning çok iyi çalışmış durumda\n",
        "\n",
        "Bu modeli “baseline final model” olarak kabul edebiliriz."
      ],
      "metadata": {
        "id": "wZzwp7lEgg44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEST LGBM MODEL LOAD + PREDICT"
      ],
      "metadata": {
        "id": "n1RBSLohszqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# LOAD BEST LIGHTGBM MODEL & PREDICT (FIXED VERSION)\n",
        "# ==========================================================\n",
        "\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# 1) LOAD best model\n",
        "model_path = f\"{BASE}/lgbm_best_model.txt\"\n",
        "best_model = lgb.Booster(model_file=model_path)\n",
        "\n",
        "print(\"Best model loaded from:\")\n",
        "print(model_path)\n",
        "\n",
        "# 2) LOAD test dataset\n",
        "test_final = pd.read_parquet(f\"{BASE}/final_test_reduced.parquet\")\n",
        "\n",
        "# 3) Get training feature list from model\n",
        "train_features = best_model.feature_name()\n",
        "print(\"Model feature count:\", len(train_features))\n",
        "\n",
        "# 4) Align test dataset to training features\n",
        "X_test = test_final.reindex(columns=train_features, fill_value=0)\n",
        "\n",
        "print(\"Test feature count after alignment:\", X_test.shape[1])\n",
        "\n",
        "# 5) Predict\n",
        "test_preds = best_model.predict(X_test)\n",
        "\n",
        "# 6) Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"SK_ID_CURR\": test_final[\"SK_ID_CURR\"],\n",
        "    \"TARGET\": test_preds\n",
        "})\n",
        "submission.to_csv(f\"{BASE}/submission_lgbm_best.csv\", index=False)\n",
        "\n",
        "print(\"\\nSaved submission_lgbm_best.csv\")\n",
        "print(\"=== DONE ===\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YPB2HxbGi46l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CATBOOST STRATIFIED K-FOLD FINAL MODEL\n"
      ],
      "metadata": {
        "id": "26eSEfpPgztE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CATBOOST K-FOLD TRAINING + BEST MODEL SELECTION (FULL PIPELINE)\n",
        "# ==========================================================\n",
        "\n",
        "!pip install catboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Load dataset\n",
        "# ----------------------------------------------------------\n",
        "train_final = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "test_final  = pd.read_parquet(f\"{BASE}/final_test_reduced.parquet\")\n",
        "\n",
        "y = train_final[\"TARGET\"].values\n",
        "X = train_final.drop(columns=[\"TARGET\"])\n",
        "\n",
        "print(\"Train:\", X.shape, \" Test:\", test_final.shape)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# CatBoost parameters\n",
        "# ----------------------------------------------------------\n",
        "cat_params = {\n",
        "    \"loss_function\": \"Logloss\",\n",
        "    \"eval_metric\": \"AUC\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"depth\": 8,\n",
        "    \"l2_leaf_reg\": 3,\n",
        "    \"random_strength\": 1,\n",
        "    \"bagging_temperature\": 0.5,\n",
        "    \"random_seed\": 42,\n",
        "    \"verbose\": 200\n",
        "}\n",
        "\n",
        "print(\"\\nTraining CatBoost 5-Fold and saving fold models...\\n\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# CHUNK 1 — Train 5 folds and save each fold model\n",
        "# ----------------------------------------------------------\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), 1):\n",
        "\n",
        "    print(f\"\\n===== FOLD {fold} =====\")\n",
        "\n",
        "    train_pool = Pool(X.iloc[train_idx], y[train_idx])\n",
        "    valid_pool = Pool(X.iloc[valid_idx], y[valid_idx])\n",
        "\n",
        "    model = CatBoostClassifier(**cat_params)\n",
        "\n",
        "    model.fit(\n",
        "        train_pool,\n",
        "        eval_set=valid_pool,\n",
        "        use_best_model=True\n",
        "    )\n",
        "\n",
        "    # SAVE fold model\n",
        "    fold_path = f\"{BASE}/catboost_model_fold{fold}.cbm\"\n",
        "    model.save_model(fold_path)\n",
        "\n",
        "    print(f\"Saved fold model: {fold_path}\")\n",
        "\n",
        "print(\"\\n=== ALL CATBOOST FOLD MODELS SAVED SUCCESSFULLY ===\\n\")\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# CHUNK 2 — SELECT BEST MODEL FROM DISK\n",
        "# ==========================================================\n",
        "\n",
        "print(\"Evaluating fold models to select BEST model...\\n\")\n",
        "\n",
        "best_auc = -1\n",
        "best_fold = None\n",
        "best_model = None\n",
        "\n",
        "for fold in range(1, 6):\n",
        "\n",
        "    model_path = f\"{BASE}/catboost_model_fold{fold}.cbm\"\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Missing:\", model_path)\n",
        "        continue\n",
        "\n",
        "    print(f\"Loading: {model_path}\")\n",
        "    model = CatBoostClassifier()\n",
        "    model.load_model(model_path)\n",
        "\n",
        "    preds = model.predict_proba(X)[:, 1]\n",
        "    auc = roc_auc_score(y, preds)\n",
        "\n",
        "    print(f\"Fold {fold} AUC: {auc:.6f}\")\n",
        "\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        best_fold = fold\n",
        "        best_model = model\n",
        "\n",
        "print(\"\\n===================================\")\n",
        "print(\"BEST FOLD:\", best_fold)\n",
        "print(\"BEST AUC :\", best_auc)\n",
        "print(\"===================================\\n\")\n",
        "\n",
        "# Save BEST model\n",
        "best_path = f\"{BASE}/catboost_best_model.cbm\"\n",
        "best_model.save_model(best_path)\n",
        "print(f\"Saved BEST CatBoost model to: {best_path}\")\n",
        "\n",
        "# Delete other fold models\n",
        "print(\"\\nDeleting other fold models...\\n\")\n",
        "for fold in range(1, 6):\n",
        "    path = f\"{BASE}/catboost_model_fold{fold}.cbm\"\n",
        "    if fold != best_fold and os.path.exists(path):\n",
        "        os.remove(path)\n",
        "        print(\"Deleted:\", path)\n",
        "\n",
        "print(\"\\n=== BEST CATBOOST MODEL SAVED & OTHER MODELS REMOVED ===\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nPh4CJJOYmx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBOOST"
      ],
      "metadata": {
        "id": "RYXJ0w6GHR3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# XGBOOST RAM-SAFE FULL PIPELINE (3-FOLD, approx)\n",
        "# ==========================================================\n",
        "\n",
        "!pip install xgboost\n",
        "\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os, gc\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "train_final = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "test_final  = pd.read_parquet(f\"{BASE}/final_test_reduced.parquet\")\n",
        "\n",
        "y = train_final[\"TARGET\"].values\n",
        "X = train_final.drop(columns=[\"TARGET\"])\n",
        "X_test = test_final[X.columns]\n",
        "\n",
        "print(\"Train:\", X.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "# Memory-friendly params\n",
        "xgb_params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"auc\",\n",
        "    \"eta\": 0.05,\n",
        "    \"max_depth\": 6,\n",
        "    \"subsample\": 0.7,\n",
        "    \"colsample_bytree\": 0.7,\n",
        "    \"colsample_bylevel\": 0.7,\n",
        "    \"lambda\": 2.0,\n",
        "    \"alpha\": 0.5,\n",
        "    \"tree_method\": \"approx\",\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold_scores = []\n",
        "best_auc = -1\n",
        "best_fold = None\n",
        "best_model_path = None\n",
        "\n",
        "print(\"\\nTraining XGBoost (RAM-SAFE)...\\n\")\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), 1):\n",
        "\n",
        "    print(f\"\\n===== FOLD {fold} =====\")\n",
        "\n",
        "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train.astype(np.float32), label=y_train)\n",
        "    dvalid = xgb.DMatrix(X_valid.astype(np.float32), label=y_valid)\n",
        "\n",
        "    model = xgb.train(\n",
        "        params=xgb_params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=800,\n",
        "        evals=[(dvalid, \"valid\")],\n",
        "        early_stopping_rounds=60,\n",
        "        verbose_eval=200\n",
        "    )\n",
        "\n",
        "    preds_valid = model.predict(dvalid)\n",
        "    fold_auc = roc_auc_score(y_valid, preds_valid)\n",
        "    fold_scores.append(fold_auc)\n",
        "    print(f\"Fold {fold} AUC: {fold_auc:.6f}\")\n",
        "\n",
        "    fold_path = f\"{BASE}/xgb_model_fold{fold}.json\"\n",
        "    model.save_model(fold_path)\n",
        "    print(\"Saved:\", fold_path)\n",
        "\n",
        "    if fold_auc > best_auc:\n",
        "        best_auc = fold_auc\n",
        "        best_fold = fold\n",
        "        best_model_path = fold_path\n",
        "\n",
        "    del dtrain, dvalid, model\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nBEST FOLD:\", best_fold, \"AUC:\", best_auc)\n",
        "\n",
        "# Load best model\n",
        "best_model = xgb.Booster()\n",
        "best_model.load_model(best_model_path)\n",
        "\n",
        "best_model.save_model(f\"{BASE}/xgboost_best_model.json\")\n",
        "print(\"Saved BEST model.\")\n",
        "\n",
        "# Clean other models\n",
        "for fold in range(1, 4):\n",
        "    fp = f\"{BASE}/xgb_model_fold{fold}.json\"\n",
        "    if fp != best_model_path and os.path.exists(fp):\n",
        "        os.remove(fp)\n",
        "\n",
        "# Prediction\n",
        "dtest = xgb.DMatrix(X_test.astype(np.float32))\n",
        "test_preds = best_model.predict(dtest)\n",
        "\n",
        "submission = pd.DataFrame({\"SK_ID_CURR\": test_final[\"SK_ID_CURR\"], \"TARGET\": test_preds})\n",
        "submission.to_csv(f\"{BASE}/submission_xgboost_best.csv\", index=False)\n",
        "\n",
        "print(\"\\n=== XGBOOST RAM-SAFE PIPELINE DONE ===\")\n"
      ],
      "metadata": {
        "id": "46m_OOjcEh8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna for Catboost"
      ],
      "metadata": {
        "id": "2H78N4Eg8NGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna catboost\n",
        "\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# ============================================================\n",
        "# Load data\n",
        "# ============================================================\n",
        "train_final = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "\n",
        "y = train_final[\"TARGET\"].values\n",
        "X = train_final.drop(columns=[\"TARGET\"])\n",
        "\n",
        "print(\"Train:\", X.shape)\n",
        "\n",
        "# ============================================================\n",
        "# Optuna Objective (FULL 5-FOLD)\n",
        "# ============================================================\n",
        "def objective(trial):\n",
        "\n",
        "    # Hyperparameter search space\n",
        "    params = {\n",
        "        \"loss_function\": \"Logloss\",\n",
        "        \"eval_metric\": \"AUC\",\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.12),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 5, 10),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 8.0),\n",
        "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
        "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.5, 2.5),\n",
        "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
        "        \"iterations\": trial.suggest_int(\"iterations\", 500, 1200),\n",
        "        \"random_seed\": 42,\n",
        "        \"verbose\": False\n",
        "    }\n",
        "\n",
        "    # 5-fold CV evaluation\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_aucs = []\n",
        "\n",
        "    for tr_idx, val_idx in kf.split(X, y):\n",
        "        X_train, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y[tr_idx], y[val_idx]\n",
        "\n",
        "        train_pool = Pool(X_train, y_train)\n",
        "        valid_pool = Pool(X_val, y_val)\n",
        "\n",
        "        model = CatBoostClassifier(**params)\n",
        "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
        "\n",
        "        preds = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, preds)\n",
        "        fold_aucs.append(auc)\n",
        "\n",
        "        # RAM cleanup\n",
        "        del model, preds, train_pool, valid_pool\n",
        "        gc.collect()\n",
        "\n",
        "    # Return mean AUC\n",
        "    mean_auc = np.mean(fold_aucs)\n",
        "    return mean_auc\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Run Optuna Study (15–20 trials recommended)\n",
        "# ============================================================\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)   # 20 trial ideal\n",
        "\n",
        "print(\"\\n===== OPTUNA COMPLETED =====\")\n",
        "print(\"Best AUC:\", study.best_value)\n",
        "print(\"Best Params:\", study.best_params)\n",
        "\n",
        "# Save best params to disk\n",
        "import json\n",
        "with open(f\"{BASE}/catboost_optuna_5fold_params.json\", \"w\") as f:\n",
        "    json.dump(study.best_params, f, indent=4)\n",
        "\n",
        "print(\"\\nSaved best params JSON to:\")\n",
        "print(f\"{BASE}/catboost_optuna_5fold_params.json\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EcAKjsy0WuLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  CatBoost final model training with optuna best parameters"
      ],
      "metadata": {
        "id": "kZA57ZPbZ7WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "import gc\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# =====================================================\n",
        "# LOAD DATA\n",
        "# =====================================================\n",
        "train_final = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "\n",
        "y = train_final[\"TARGET\"].values\n",
        "X = train_final.drop(columns=[\"TARGET\"])\n",
        "\n",
        "print(\"Train:\", X.shape)\n",
        "\n",
        "# =====================================================\n",
        "# OPTUNA BEST PARAMS (your results)\n",
        "# =====================================================\n",
        "best_params = {\n",
        "    \"loss_function\": \"Logloss\",\n",
        "    \"eval_metric\": \"AUC\",\n",
        "    \"learning_rate\": 0.10366541399501397,\n",
        "    \"depth\": 5,\n",
        "    \"l2_leaf_reg\": 6.049887172004972,\n",
        "    \"bagging_temperature\": 0.37262519132557553,\n",
        "    \"random_strength\": 0.8461100871182015,\n",
        "    \"border_count\": 61,\n",
        "    \"iterations\": 892,\n",
        "    \"random_seed\": 42,\n",
        "    \"verbose\": 200\n",
        "}\n",
        "\n",
        "print(\"\\nUsing Optuna best params:\\n\", best_params)\n",
        "\n",
        "# =====================================================\n",
        "# 5-FOLD FINAL TRAINING\n",
        "# =====================================================\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_aucs = []\n",
        "best_auc = -1\n",
        "best_fold = None\n",
        "best_model = None\n",
        "\n",
        "print(\"\\n===== STARTING 5-FOLD FINAL TRAINING =====\\n\")\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), 1):\n",
        "\n",
        "    print(f\"\\n========== FOLD {fold} ==========\")\n",
        "\n",
        "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
        "\n",
        "    train_pool = Pool(X_train, y_train)\n",
        "    valid_pool = Pool(X_valid, y_valid)\n",
        "\n",
        "    model = CatBoostClassifier(**best_params)\n",
        "\n",
        "    model.fit(\n",
        "        train_pool,\n",
        "        eval_set=valid_pool,\n",
        "        use_best_model=True\n",
        "    )\n",
        "\n",
        "    preds = model.predict_proba(X_valid)[:, 1]\n",
        "    fold_auc = roc_auc_score(y_valid, preds)\n",
        "\n",
        "    fold_aucs.append(fold_auc)\n",
        "\n",
        "    print(f\"\\nFOLD {fold} AUC: {fold_auc:.6f}\")\n",
        "\n",
        "    # Save fold model\n",
        "    model_path = f\"{BASE}/cat_optuna_fold{fold}.cbm\"\n",
        "    model.save_model(model_path)\n",
        "    print(\"Saved fold model:\", model_path)\n",
        "\n",
        "    # Track best model\n",
        "    if fold_auc > best_auc:\n",
        "        best_auc = fold_auc\n",
        "        best_fold = fold\n",
        "        best_model = model\n",
        "\n",
        "    # Clean RAM\n",
        "    del model, preds, train_pool, valid_pool\n",
        "    gc.collect()\n",
        "\n",
        "# =====================================================\n",
        "# SAVE BEST MODEL\n",
        "# =====================================================\n",
        "print(\"\\n=======================================\")\n",
        "print(\"5-FOLD AUCs:\", fold_aucs)\n",
        "print(\"MEAN AUC:\", np.mean(fold_aucs))\n",
        "print(\"BEST FOLD:\", best_fold)\n",
        "print(\"BEST FOLD AUC:\", best_auc)\n",
        "print(\"=======================================\\n\")\n",
        "\n",
        "best_path = f\"{BASE}/catboost_optuna_5fold_best_model.cbm\"\n",
        "best_model.save_model(best_path)\n",
        "print(\"Saved BEST model to:\", best_path)\n",
        "\n",
        "# =====================================================\n",
        "# DELETE OTHER FOLD MODELS\n",
        "# =====================================================\n",
        "print(\"\\nDeleting other fold models...\\n\")\n",
        "for fold in range(1, 6):\n",
        "    path = f\"{BASE}/cat_optuna_fold{fold}.cbm\"\n",
        "    if fold != best_fold and os.path.exists(path):\n",
        "        os.remove(path)\n",
        "        print(\"Deleted:\", path)\n",
        "\n",
        "print(\"\\n===== TRAINING COMPLETE — BEST MODEL SAVED =====\\n\")\n"
      ],
      "metadata": {
        "id": "yQtW8a4PWuWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJb6MwJ5XRA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blending (CATBOOST, LightGBM, XGBoost)"
      ],
      "metadata": {
        "id": "Xw2XSBGHI4I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "import pandas as pd\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# Load test dataset\n",
        "test_final = pd.read_parquet(f\"{BASE}/final_test_reduced.parquet\")\n",
        "\n",
        "# Load CatBoost model (correct filename)\n",
        "cat_model = CatBoostClassifier()\n",
        "cat_model.load_model(f\"{BASE}/catboost_model_fold5.cbm\")\n",
        "\n",
        "pred_cat = cat_model.predict_proba(test_final)[:, 1]\n",
        "print(\"CatBoost preds:\", pred_cat[:5])\n",
        "from catboost import CatBoostClassifier\n",
        "import pandas as pd\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# Load test dataset\n",
        "test_final = pd.read_parquet(f\"{BASE}/final_test_reduced.parquet\")\n",
        "\n",
        "# Load CatBoost model (correct filename)\n",
        "cat_model = CatBoostClassifier()\n",
        "cat_model.load_model(f\"{BASE}/catboost_model_fold5.cbm\")\n",
        "\n",
        "pred_cat = cat_model.predict_proba(test_final)[:, 1]\n",
        "print(\"CatBoost preds:\", pred_cat[:5])\n"
      ],
      "metadata": {
        "id": "kmQaYaSLJAQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ligthgbm\n",
        "\n",
        "!pip install lightgbm\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "lgb_model = lgb.Booster(model_file=f\"{BASE}/lgbm_best_model.txt\")\n",
        "pred_lgb = lgb_model.predict(test_final)\n",
        "\n",
        "print(\"LGBM preds:\", pred_lgb[:5])\n"
      ],
      "metadata": {
        "id": "RpROnKfaKyj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "!pip install xgboost\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb_model = xgb.Booster()\n",
        "xgb_model.load_model(f\"{BASE}/xgboost_best_model.json\")\n",
        "\n",
        "dtest = xgb.DMatrix(test_final)\n",
        "pred_xgb = xgb_model.predict(dtest)\n",
        "\n",
        "print(pred_xgb[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "tK45pkp2LWJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blending Results\n",
        "\n",
        "# ==========================================================\n",
        "# OOF BLENDING PREDICTIONS (HACK VERSION FOR ALL MODELS)\n",
        "# ==========================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "import gc\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# Load data\n",
        "train = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "y = train[\"TARGET\"].values\n",
        "X = train.drop(columns=[\"TARGET\"])\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Empty OOF vectors\n",
        "oof_cat = np.zeros(len(X))\n",
        "oof_lgb = np.zeros(len(X))\n",
        "oof_xgb = np.zeros(len(X))\n",
        "\n",
        "print(\"Starting OOF predictions (ALL MODELS USE FINAL VERSION).\")\n",
        "\n",
        "# ==================================\n",
        "# LOAD ALL THREE MODELS ONLY ONCE\n",
        "# ==================================\n",
        "\n",
        "# CatBoost final model\n",
        "cat = CatBoostClassifier()\n",
        "cat.load_model(f\"{BASE}/catboost_model_fold5.cbm\")\n",
        "\n",
        "# LightGBM final model\n",
        "lgb_model = lgb.Booster(model_file=f\"{BASE}/lgbm_best_model.txt\")\n",
        "\n",
        "# XGBoost final model\n",
        "xgb_model = xgb.Booster()\n",
        "xgb_model.load_model(f\"{BASE}/xgboost_best_model.json\")\n",
        "\n",
        "\n",
        "fold_num = 1\n",
        "for tr_idx, val_idx in kf.split(X, y):\n",
        "\n",
        "    print(f\"===== FOLD {fold_num} =====\")\n",
        "\n",
        "    X_val = X.iloc[val_idx]\n",
        "\n",
        "    # ---- CatBoost ----\n",
        "    oof_cat[val_idx] = cat.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    # ---- LightGBM ----\n",
        "    oof_lgb[val_idx] = lgb_model.predict(X_val.values)\n",
        "\n",
        "    # ---- XGBoost ----\n",
        "    dval = xgb.DMatrix(X_val)\n",
        "    oof_xgb[val_idx] = xgb_model.predict(dval)\n",
        "\n",
        "    fold_num += 1\n",
        "    del dval\n",
        "    gc.collect()\n",
        "\n",
        "# Save OOF predictions\n",
        "oof_df = pd.DataFrame({\n",
        "    \"oof_cat\": oof_cat,\n",
        "    \"oof_lgb\": oof_lgb,\n",
        "    \"oof_xgb\": oof_xgb,\n",
        "    \"TARGET\": y\n",
        "})\n",
        "\n",
        "oof_df.to_csv(f\"{BASE}/oof_predictions_hack.csv\", index=False)\n",
        "print(\"\\nOOF predictions saved → oof_predictions_hack.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YivLM54MN1Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimal Blending"
      ],
      "metadata": {
        "id": "pAC5Z9-5P7jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CHUNK 2 — BLENDING WEIGHT OPTIMIZATION (GRID SEARCH)\n",
        "# ==========================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# Load OOF predictions\n",
        "df = pd.read_csv(f\"{BASE}/oof_predictions_hack.csv\")\n",
        "\n",
        "print(\"OOF file loaded:\", df.shape)\n",
        "\n",
        "oof_cat = df[\"oof_cat\"].values\n",
        "oof_lgb = df[\"oof_lgb\"].values\n",
        "oof_xgb = df[\"oof_xgb\"].values\n",
        "y_true  = df[\"TARGET\"].values\n",
        "\n",
        "best_auc = -1\n",
        "best_weights = None\n",
        "results = []\n",
        "\n",
        "# Weight search space\n",
        "cat_weights = [0.50, 0.60, 0.65, 0.70, 0.75]\n",
        "lgb_weights = [0.10, 0.15, 0.20, 0.25]\n",
        "# xgb_weight = 1 - (cat + lgb)\n",
        "\n",
        "print(\"\\nStarting weight optimization...\\n\")\n",
        "\n",
        "for w_cat in cat_weights:\n",
        "    for w_lgb in lgb_weights:\n",
        "        w_xgb = 1 - (w_cat + w_lgb)\n",
        "        if w_xgb < 0:\n",
        "            continue  # skip invalid combinations\n",
        "\n",
        "        # Blend OOF\n",
        "        pred = w_cat*oof_cat + w_lgb*oof_lgb + w_xgb*oof_xgb\n",
        "\n",
        "        auc = roc_auc_score(y_true, pred)\n",
        "        results.append((w_cat, w_lgb, w_xgb, auc))\n",
        "\n",
        "        print(f\"Weights → Cat={w_cat}, LGB={w_lgb}, XGB={w_xgb:.2f}, AUC={auc:.6f}\")\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_weights = (w_cat, w_lgb, w_xgb)\n",
        "\n",
        "print(\"\\n=============================\")\n",
        "print(\" BEST BLENDING RESULTS\")\n",
        "print(\"=============================\")\n",
        "print(f\"Best AUC:     {best_auc:.6f}\")\n",
        "print(f\"Best Weights: Cat={best_weights[0]}, LGB={best_weights[1]}, XGB={best_weights[2]}\")\n",
        "print(\"=============================\\n\")\n",
        "\n",
        "# Save weights & results\n",
        "pd.DataFrame(results, columns=[\"cat\", \"lgb\", \"xgb\", \"auc\"]).to_csv(f\"{BASE}/blending_weight_results.csv\", index=False)\n",
        "\n",
        "with open(f\"{BASE}/best_blending_weights.txt\", \"w\") as f:\n",
        "    f.write(f\"Best AUC={best_auc}\\n\")\n",
        "    f.write(f\"Weights: Cat={best_weights[0]}, LGB={best_weights[1]}, XGB={best_weights[2]}\\n\")\n",
        "\n",
        "print(\"Saved results to disk.\")\n"
      ],
      "metadata": {
        "id": "YW9j9NqPP9Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OOF AUC Scores"
      ],
      "metadata": {
        "id": "sVMBNbOrbseX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# Load OOF file\n",
        "df = pd.read_csv(f\"{BASE}/oof_predictions_hack.csv\")\n",
        "\n",
        "print(\"OOF shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Compute OOF AUC values for base models\n",
        "auc_cat_oof = roc_auc_score(df[\"TARGET\"], df[\"oof_cat\"])\n",
        "auc_lgb_oof = roc_auc_score(df[\"TARGET\"], df[\"oof_lgb\"])\n",
        "auc_xgb_oof = roc_auc_score(df[\"TARGET\"], df[\"oof_xgb\"])\n",
        "\n",
        "print(\"\\n===================\")\n",
        "print(\"OOF AUC SCORES\")\n",
        "print(\"===================\")\n",
        "print(\"CatBoost OOF AUC :\", auc_cat_oof)\n",
        "print(\"LightGBM OOF AUC:\", auc_lgb_oof)\n",
        "print(\"XGBoost OOF AUC :\", auc_xgb_oof)\n"
      ],
      "metadata": {
        "id": "sVkpagfjbH3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blending or stacking"
      ],
      "metadata": {
        "id": "8olYicG5dQnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# FINAL ENSEMBLE CHUNK — BLENDING + STACKING + SAVE OOF FILES\n",
        "# ==========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# --------------------------\n",
        "# Load OOF predictions\n",
        "# --------------------------\n",
        "df = pd.read_csv(f\"{BASE}/oof_predictions_hack.csv\")\n",
        "print(\"Loaded OOF predictions:\", df.shape)\n",
        "\n",
        "# ==========================================================\n",
        "# BLENDING\n",
        "# ==========================================================\n",
        "\n",
        "# Load best blending weights\n",
        "with open(f\"{BASE}/best_blending_weights.txt\", \"r\") as f:\n",
        "    line = [l for l in f.readlines() if \"Weights:\" in l][0]\n",
        "\n",
        "parts = line.replace(\"Weights:\", \"\").strip().split(\",\")\n",
        "w_cat = float(parts[0].split(\"=\")[1])\n",
        "w_lgb = float(parts[1].split(\"=\")[1])\n",
        "w_xgb = float(parts[2].split(\"=\")[1])\n",
        "\n",
        "print(\"Using blending weights:\", w_cat, w_lgb, w_xgb)\n",
        "\n",
        "# Compute blended prediction\n",
        "blend_oof = (\n",
        "    w_cat * df[\"oof_cat\"] +\n",
        "    w_lgb * df[\"oof_lgb\"] +\n",
        "    w_xgb * df[\"oof_xgb\"]\n",
        ")\n",
        "\n",
        "y_true = df[\"TARGET\"].values\n",
        "auc_blend = roc_auc_score(y_true, blend_oof)\n",
        "print(\"Blending OOF AUC:\", auc_blend)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# STACKING (Logistic Regression)\n",
        "# ==========================================================\n",
        "\n",
        "X_meta = df[[\"oof_cat\", \"oof_lgb\", \"oof_xgb\"]].values\n",
        "\n",
        "stack_model = LogisticRegression(max_iter=10000)\n",
        "stack_model.fit(X_meta, y_true)\n",
        "\n",
        "# Generate stacking OOF predictions\n",
        "stack_oof_pred = stack_model.predict_proba(X_meta)[:, 1]\n",
        "auc_stack = roc_auc_score(y_true, stack_oof_pred)\n",
        "print(\"Stacking OOF AUC:\", auc_stack)\n",
        "\n",
        "# Save stacking model\n",
        "model_path = f\"{BASE}/stacking_lr_model.pkl\"\n",
        "joblib.dump(stack_model, model_path)\n",
        "print(\"Saved stacking model →\", model_path)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# SAVE stacking_oof_predictions.csv\n",
        "# ==========================================================\n",
        "\n",
        "df_out = df.copy()\n",
        "df_out[\"blend_pred\"] = blend_oof\n",
        "df_out[\"stack_pred\"] = stack_oof_pred\n",
        "\n",
        "stacking_oof_path = f\"{BASE}/stacking_oof_predictions.csv\"\n",
        "df_out.to_csv(stacking_oof_path, index=False)\n",
        "\n",
        "print(\"Saved stacking OOF predictions →\", stacking_oof_path)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# COMPARE ENSEMBLE METHODS\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n======================================\")\n",
        "print(\"MODEL ENSEMBLE COMPARISON (OOF AUC)\")\n",
        "print(\"--------------------------------------\")\n",
        "print(f\"Blending OOF AUC : {auc_blend:.6f}\")\n",
        "print(f\"Stacking OOF AUC : {auc_stack:.6f}\")\n",
        "\n",
        "best_model = \"STACKING\" if auc_stack > auc_blend else \"BLENDING\"\n",
        "print(\"\\nBEST ENSEMBLE MODEL:\", best_model)\n",
        "print(\"======================================\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xdxVAwAuYODj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# MULTI-LINE MODEL PERFORMANCE CHART — CLEAN, PDF-SAFE, FINAL VERSION\n",
        "# ===================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# ===========================\n",
        "# MODEL NAMES + AUC VALUES\n",
        "# ===========================\n",
        "model_names = [\"CatBoost\", \"LightGBM\", \"XGBoost\", \"Blending\", \"Stacking\"]\n",
        "\n",
        "auc_scores = [\n",
        "    auc_cat_oof,\n",
        "    auc_lgb_oof,\n",
        "    auc_xgb_oof,\n",
        "    auc_blend,\n",
        "    auc_stack\n",
        "]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "\n",
        "# ===========================\n",
        "# COLORS & MARKERS (PDF SAFE)\n",
        "# ===========================\n",
        "colors = [\"#FF6B6B\", \"#1E90FF\", \"#FFA534\", \"#6CC24A\", \"#9B59B6\"]\n",
        "markers = [\"o\", \"s\", \"D\", \"^\", \"X\"]\n",
        "\n",
        "# Create figure\n",
        "plt.figure(figsize=(12, 6), dpi=300, facecolor=\"white\")\n",
        "\n",
        "# ===========================\n",
        "# DRAW INDIVIDUAL MODEL LINES\n",
        "# (Each model has its own colored line segment)\n",
        "# ===========================\n",
        "for i in range(len(model_names)):\n",
        "    plt.plot(\n",
        "        [x[i], x[i]],  # single x position (vertical micro-line)\n",
        "        [auc_scores[i] - 0.0005, auc_scores[i] + 0.0005],  # tiny vertical line (banking style)\n",
        "        color=colors[i],\n",
        "        linewidth=3\n",
        "    )\n",
        "    plt.scatter(\n",
        "        x[i], auc_scores[i],\n",
        "        color=colors[i],\n",
        "        marker=markers[i],\n",
        "        s=150,\n",
        "        label=f\"{model_names[i]} (AUC={auc_scores[i]:.4f})\"\n",
        "    )\n",
        "\n",
        "# ===========================\n",
        "# CONNECT POINTS WITH NEUTRAL LINE\n",
        "# ===========================\n",
        "plt.plot(\n",
        "    x,\n",
        "    auc_scores,\n",
        "    color=\"#444444\",\n",
        "    linewidth=1.5,\n",
        "    linestyle=\"--\",\n",
        "    alpha=0.4\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# ANNOTATIONS\n",
        "# ===========================\n",
        "for i, score in enumerate(auc_scores):\n",
        "    plt.text(\n",
        "        x[i],\n",
        "        score + 0.002,\n",
        "        f\"{score:.4f}\",\n",
        "        ha=\"center\",\n",
        "        fontsize=10,\n",
        "        fontweight=\"bold\",\n",
        "        color=colors[i]\n",
        "    )\n",
        "\n",
        "# ===========================\n",
        "# LABELS & TITLES\n",
        "# ===========================\n",
        "plt.xticks(x, model_names, fontsize=11)\n",
        "plt.ylabel(\"AUC Score\", fontsize=12)\n",
        "plt.title(\"Model Performance Comparison (OOF AUC Scores)\", fontsize=15, fontweight=\"bold\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
        "plt.ylim(min(auc_scores) - 0.01, max(auc_scores) + 0.02)\n",
        "\n",
        "plt.legend(fontsize=10, loc=\"lower right\", frameon=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "save_path = f\"{BASE}/model_performance_multiline.png\"\n",
        "plt.savefig(save_path, dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved →\", save_path)\n"
      ],
      "metadata": {
        "id": "15d53b-KikkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking OOF"
      ],
      "metadata": {
        "id": "CfoJRP0HqUkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# FULL STACKING GRAPH + REPORTLAB PDF GENERATOR (FINAL VERSION)\n",
        "# ===================================================================\n",
        "\n",
        "!pip install reportlab\n",
        "\n",
        "# ===================================================================\n",
        "# IMPORTS\n",
        "# ===================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc, roc_auc_score\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "from reportlab.platypus import SimpleDocTemplate, Image, Paragraph, Spacer\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# Global PDF-safe settings\n",
        "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
        "plt.rcParams[\"savefig.facecolor\"] = \"white\"\n",
        "plt.rcParams[\"font.size\"] = 11\n",
        "\n",
        "# ===================================================================\n",
        "# FUNCTIONS: SAVE & SHOW\n",
        "# ===================================================================\n",
        "\n",
        "# Save high-resolution figure for PDF\n",
        "def save_fig(path):\n",
        "    plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "# Show small preview in Colab\n",
        "def show_fig():\n",
        "    plt.gcf().set_size_inches(5, 4)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# ===================================================================\n",
        "# LOAD OOF DATA\n",
        "# ===================================================================\n",
        "df = pd.read_csv(f\"{BASE}/stacking_oof_predictions.csv\")\n",
        "\n",
        "y_true = df[\"TARGET\"].values\n",
        "stack_pred = df[\"stack_pred\"].values\n",
        "oof_cat = df[\"oof_cat\"].values\n",
        "oof_lgb = df[\"oof_lgb\"].values\n",
        "oof_xgb = df[\"oof_xgb\"].values\n",
        "\n",
        "# Blending (optional, for comparison graph)\n",
        "w_cat, w_lgb, w_xgb = 0.50, 0.25, 0.25\n",
        "blend_pred = w_cat * oof_cat + w_lgb * oof_lgb + w_xgb * oof_xgb\n",
        "\n",
        "# ===================================================================\n",
        "# KS SCORE\n",
        "# ===================================================================\n",
        "def ks_score(y, pred):\n",
        "    data = pd.DataFrame({\"y\": y, \"pred\": pred}).sort_values(\"pred\")\n",
        "    cum_bad = (data[\"y\"] == 1).cumsum() / (data[\"y\"] == 1).sum()\n",
        "    cum_good = (data[\"y\"] == 0).cumsum() / (data[\"y\"] == 0).sum()\n",
        "    return max(abs(cum_bad - cum_good))\n",
        "\n",
        "ks = ks_score(y_true, stack_pred)\n",
        "print(\"STACKING KS SCORE:\", ks)\n",
        "\n",
        "# ===================================================================\n",
        "# 1) ROC Curve\n",
        "# ===================================================================\n",
        "fpr, tpr, _ = roc_curve(y_true, stack_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.plot(fpr, tpr, color=\"#1f77b4\", linewidth=2.5, label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.title(\"ROC Curve – Stacking Model\", fontsize=13, fontweight=\"bold\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "save_fig(f\"{BASE}/ROC_STACKING.png\")\n",
        "show_fig()\n",
        "\n",
        "# ===================================================================\n",
        "# 2) PRECISION–RECALL Curve\n",
        "# ===================================================================\n",
        "precision, recall, _ = precision_recall_curve(y_true, stack_pred)\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.plot(recall, precision, color=\"#d62728\", linewidth=2.5)\n",
        "plt.title(\"Precision–Recall Curve – Stacking Model\", fontsize=13, fontweight=\"bold\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "save_fig(f\"{BASE}/PR_STACKING.png\")\n",
        "show_fig()\n",
        "\n",
        "# ===================================================================\n",
        "# 3) CALIBRATION Curve\n",
        "# ===================================================================\n",
        "prob_true, prob_pred = calibration_curve(y_true, stack_pred, n_bins=20)\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.plot(prob_pred, prob_true, \"o-\", color=\"#2ca02c\", linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.title(\"Calibration Curve – Stacking Model\", fontsize=13, fontweight=\"bold\")\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"Observed Frequency\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "save_fig(f\"{BASE}/CALIBRATION_STACKING.png\")\n",
        "show_fig()\n",
        "\n",
        "# ===================================================================\n",
        "# 4) MODEL COMPARISON (AUC OOF)\n",
        "# ===================================================================\n",
        "auc_cat_oof = roc_auc_score(y_true, oof_cat)\n",
        "auc_lgb_oof = roc_auc_score(y_true, oof_lgb)\n",
        "auc_xgb_oof = roc_auc_score(y_true, oof_xgb)\n",
        "auc_blend = roc_auc_score(y_true, blend_pred)\n",
        "auc_stack = roc_auc_score(y_true, stack_pred)\n",
        "\n",
        "model_names = [\"CatBoost\", \"LightGBM\", \"XGBoost\", \"Blending\", \"Stacking\"]\n",
        "auc_scores = [auc_cat_oof, auc_lgb_oof, auc_xgb_oof, auc_blend, auc_stack]\n",
        "colors = [\"#FF6B6B\", \"#1E90FF\", \"#FFA534\", \"#6CC24A\", \"#9B59B6\"]\n",
        "markers = [\"o\", \"s\", \"D\", \"^\", \"X\"]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(len(model_names)):\n",
        "    plt.plot([x[i], x[i]], [auc_scores[i] - 0.0005, auc_scores[i] + 0.0005],\n",
        "             color=colors[i], linewidth=3)\n",
        "    plt.scatter(x[i], auc_scores[i], color=colors[i], marker=markers[i], s=160,\n",
        "                label=f\"{model_names[i]} (AUC={auc_scores[i]:.4f})\")\n",
        "\n",
        "plt.plot(x, auc_scores, color=\"#555\", linewidth=1.5, linestyle=\"--\", alpha=0.5)\n",
        "\n",
        "for i, score in enumerate(auc_scores):\n",
        "    plt.text(x[i], score + 0.002, f\"{score:.4f}\", ha=\"center\",\n",
        "             fontsize=10, fontweight=\"bold\", color=colors[i])\n",
        "\n",
        "plt.xticks(x, model_names)\n",
        "plt.ylabel(\"AUC Score\")\n",
        "plt.title(\"Model Performance Comparison (OOF AUC Scores)\", fontsize=15, fontweight=\"bold\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "plt.legend(fontsize=9, loc=\"lower right\", frameon=True)\n",
        "plt.ylim(min(auc_scores) - 0.01, max(auc_scores) + 0.02)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_fig(f\"{BASE}/MODEL_COMPARISON_STACKING.png\")\n",
        "show_fig()\n",
        "\n",
        "print(\"\\nAll STACKING-based graphs created (PNG + shown).\")\n",
        "\n",
        "# ===================================================================\n",
        "# 5) REPORTLAB PDF REPORT\n",
        "# ===================================================================\n",
        "pdf_path = f\"{BASE}/stacking_report.pdf\"\n",
        "doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "styles = getSampleStyleSheet()\n",
        "story = []\n",
        "\n",
        "story.append(Paragraph(\"<b>Stacking Model Evaluation Report</b>\", styles[\"Title\"]))\n",
        "story.append(Spacer(1, 20))\n",
        "\n",
        "def add_image(title, path):\n",
        "    story.append(Paragraph(f\"<b>{title}</b>\", styles[\"Heading2\"]))\n",
        "    story.append(Spacer(1, 10))\n",
        "    img = Image(path, width=500, height=350)\n",
        "    story.append(img)\n",
        "    story.append(Spacer(1, 30))\n",
        "\n",
        "add_image(\"ROC Curve\", f\"{BASE}/ROC_STACKING.png\")\n",
        "add_image(\"Precision–Recall Curve\", f\"{BASE}/PR_STACKING.png\")\n",
        "add_image(\"Calibration Curve\", f\"{BASE}/CALIBRATION_STACKING.png\")\n",
        "add_image(\"Model Comparison\", f\"{BASE}/MODEL_COMPARISON_STACKING.png\")\n",
        "\n",
        "doc.build(story)\n",
        "\n",
        "print(f\"PDF Report created: {pdf_path}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ogaosIC7ik8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ub72XsjmpH84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdnfW7d4pIIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files = os.listdir(BASE)\n",
        "files\n"
      ],
      "metadata": {
        "id": "xDqmTOaMwb4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "\n",
        "pdfmetrics.registerFont(TTFont('Liberation', '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4X2eTH2apIVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
        "from reportlab.lib.styles import ParagraphStyle\n",
        "from reportlab.lib.pagesizes import A4\n",
        "\n",
        "doc = SimpleDocTemplate(\"turkce_test.pdf\", pagesize=A4)\n",
        "\n",
        "style = ParagraphStyle(name=\"TR\", fontName=\"Liberation\", fontSize=14)\n",
        "story = [Paragraph(\"Türkçe karakter testi: ğüşöçıİĞÜŞÖÇ — sorun çözüldü!\", style)]\n",
        "\n",
        "doc.build(story)\n"
      ],
      "metadata": {
        "id": "bFK4Q86i0spq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "#        FINAL STACKING MODEL PDF REPORT — UTF-8 + MINIMAL COVER\n",
        "# ===================================================================\n",
        "\n",
        "!pip install reportlab\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib import colors\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "\n",
        "# =========================================================\n",
        "# 1) FONTS — TÜRKÇE KARAKTER DESTEĞİ\n",
        "# =========================================================\n",
        "\n",
        "pdfmetrics.registerFont(TTFont('Liberation', '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf'))\n",
        "\n",
        "styles = getSampleStyleSheet()\n",
        "styles.add(ParagraphStyle(name='TR', fontName='Liberation', fontSize=11))\n",
        "styles.add(ParagraphStyle(name='TitleTR', fontName='Liberation', fontSize=22, leading=26, alignment=1))\n",
        "styles.add(ParagraphStyle(name='HeaderTR', fontName='Liberation', fontSize=14, leading=18))\n",
        "\n",
        "# =========================================================\n",
        "# 2) PATHS\n",
        "# =========================================================\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "pdf_path = f\"{BASE}/HomeCredit_Stacking_Model_Report_FINAL.pdf\"\n",
        "story = []\n",
        "\n",
        "# =========================================================\n",
        "# 3) HELPERS\n",
        "# =========================================================\n",
        "\n",
        "def title(txt):\n",
        "    story.append(Paragraph(f\"<b>{txt}</b>\", styles[\"TitleTR\"]))\n",
        "    story.append(Spacer(1, 30))\n",
        "\n",
        "def header(txt):\n",
        "    story.append(Paragraph(f\"<b>{txt}</b>\", styles[\"HeaderTR\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "def text(txt):\n",
        "    story.append(Paragraph(txt, styles[\"TR\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "def add_img(path, w=480, h=280):\n",
        "    if os.path.exists(path):\n",
        "        story.append(Image(path, width=w, height=h))\n",
        "        story.append(Spacer(1, 20))\n",
        "\n",
        "def add_table(data):\n",
        "    tbl = Table(data)\n",
        "    tbl.setStyle(TableStyle([\n",
        "        (\"BACKGROUND\", (0,0), (-1,0), colors.lightgrey),\n",
        "        (\"TEXTCOLOR\", (0,0), (-1,0), colors.black),\n",
        "        (\"ALIGN\", (0,0), (-1,-1), \"CENTER\"),\n",
        "        (\"FONTNAME\", (0,0), (-1,-1), \"Liberation\"),\n",
        "        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
        "    ]))\n",
        "    story.append(tbl)\n",
        "    story.append(Spacer(1, 20))\n",
        "\n",
        "# =========================================================\n",
        "# 4) COVER PAGE (Minimal)\n",
        "# =========================================================\n",
        "\n",
        "title(\"Home Credit Default Risk — Stacking Model Değerlendirme Raporu\")\n",
        "\n",
        "text(\"\"\"\n",
        "Bu rapor, kredi skorlama sürecinde kullanılan Stacking Logistic Regression modelinin performansını,\n",
        "taban modellerle karşılaştırmasını, kalibrasyon düzeyini ve açıklanabilirliğini kapsamaktadır.\n",
        "Tüm değerlendirmeler bankacılık sektöründeki model validasyon standartlarına uygun şekilde yapılmıştır.\n",
        "\"\"\")\n",
        "\n",
        "story.append(Spacer(1, 40))\n",
        "\n",
        "text(\"<b>Hazırlayan:</b> Halim Can Albay<br/>\")\n",
        "text(\"<b>Proje:</b> Home Credit Default Risk<br/>\")\n",
        "text(\"<b>Model Tipi:</b> Stacking Logistic Regression<br/><br/>\")\n",
        "\n",
        "story.append(Spacer(1, 40))\n",
        "\n",
        "# =========================================================\n",
        "# 5) DATA OVERVIEW\n",
        "# =========================================================\n",
        "\n",
        "header(\"1. Veri Kaynakları ve Özellik Mühendisliği\")\n",
        "\n",
        "data_files = [\n",
        "    \"application_train.parquet\",\n",
        "    \"bureau_agg.parquet\",\n",
        "    \"pos_agg.parquet\",\n",
        "    \"installments_agg.parquet\",\n",
        "    \"cc_agg.parquet\",\n",
        "    \"previous_agg.parquet\",\n",
        "    \"final_train_reduced.parquet\"\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for f in data_files:\n",
        "    path = f\"{BASE}/{f}\"\n",
        "    if os.path.exists(path):\n",
        "        df_tmp = pd.read_parquet(path)\n",
        "        rows.append([f, df_tmp.shape[0], df_tmp.shape[1]])\n",
        "\n",
        "add_table([[\"Dataset\", \"Satır\", \"Sütun\"]] + rows)\n",
        "\n",
        "text(\"\"\"\n",
        "Bu veri kaynakları birleştirilerek nihai model eğitim seti elde edilmiştir.\n",
        "Korelasyon azaltma, eksik değer yönetimi ve özellik mühendisliği adımlarından sonra modellemeye hazır hale getirilmiştir.\n",
        "\"\"\")\n",
        "\n",
        "# =========================================================\n",
        "# 6) STACKING PERFORMANCE SUMMARY\n",
        "# =========================================================\n",
        "\n",
        "header(\"2. Stacking Model Performans Özeti\")\n",
        "\n",
        "ks_path = f\"{BASE}/ks_score.txt\"\n",
        "ks_score = open(ks_path).read().strip() if os.path.exists(ks_path) else \"N/A\"\n",
        "\n",
        "text(f\"\"\"\n",
        "Stacking modeli, CatBoost, LightGBM ve XGBoost taban modellerinin OOF tahminlerini meta-öğrenici olarak kullanan bir yapıdır.\n",
        "Model, güçlü bir ayrıştırma gücüne ve kalibrasyon performansına sahiptir.<br/><br/>\n",
        "<b>KS Skoru:</b> {ks_score}\n",
        "\"\"\")\n",
        "\n",
        "# =========================================================\n",
        "# 7) STACKING PERFORMANCE GRAPHS\n",
        "# =========================================================\n",
        "\n",
        "header(\"3. Performans Görselleri (Stacking)\")\n",
        "\n",
        "add_img(f\"{BASE}/ROC_STACKING.png\")\n",
        "add_img(f\"{BASE}/PR_STACKING.png\")\n",
        "add_img(f\"{BASE}/CALIBRATION_STACKING.png\")\n",
        "add_img(f\"{BASE}/MODEL_COMPARISON_STACKING.png\")\n",
        "\n",
        "text(\"\"\"\n",
        "ROC, PR ve Kalibrasyon grafikleri modelin genel ayrıştırma performansını ve olasılık tahmin gücünü doğrulamaktadır.\n",
        "Model Comparison grafiği, stacking yaklaşımının taban modellerden daha iyi bir OOF performansı sergilediğini göstermektedir.\n",
        "\"\"\")\n",
        "\n",
        "# =========================================================\n",
        "# 8) ALL MODEL COMPARISON — MULTILINE CHART\n",
        "# =========================================================\n",
        "\n",
        "header(\"4. Model Karşılaştırması — CatBoost / LGBM / XGB / Blending / Stacking\")\n",
        "\n",
        "add_img(f\"{BASE}/model_performance_multiline.png\", w=500, h=300)\n",
        "\n",
        "text(\"\"\"\n",
        "Bu grafik, farklı model ailelerinin Out-of-Fold AUC skorlarını doğrudan karşılaştırmaktadır.\n",
        "Stacking modeli genel olarak yüksek performans ve kararlılık göstermiştir.\n",
        "\"\"\")\n",
        "\n",
        "# =========================================================\n",
        "# 9) SHAP ANALYSIS\n",
        "# =========================================================\n",
        "\n",
        "header(\"5. Açıklanabilirlik Analizi (SHAP)\")\n",
        "\n",
        "shap_imgs = [\n",
        "    \"shap_summary.png\",\n",
        "    \"shap_bar.png\",\n",
        "    \"shap_waterfall_single.png\",\n",
        "    \"shap_decision_plot.png\"\n",
        "]\n",
        "\n",
        "for img in shap_imgs:\n",
        "    add_img(f\"{BASE}/{img}\", w=480, h=300)\n",
        "\n",
        "shap_csv = f\"{BASE}/shap_feature_importance.csv\"\n",
        "if os.path.exists(shap_csv):\n",
        "    df_shap = pd.read_csv(shap_csv).head(20)\n",
        "    add_table([df_shap.columns.tolist()] + df_shap.values.tolist())\n",
        "\n",
        "text(\"\"\"\n",
        "SHAP sonuçları; gelir, ödeme davranışı, yaş, kredi oranı ve geçmiş kredi performansı gibi kritik değişkenlerin\n",
        "modelin karar mekanizmasında önemli rol oynadığını göstermektedir.\n",
        "\"\"\")\n",
        "\n",
        "# =========================================================\n",
        "# 10) FINAL BUSINESS INTERPRETATION\n",
        "# =========================================================\n",
        "\n",
        "header(\"6. Sonuç ve İş Yorumlaması\")\n",
        "\n",
        "text(\"\"\"\n",
        "Stacking modeli, bankacılık kredi skorlama standartlarına uygun bir performans sergilemiştir.\n",
        "AUC, KS ve Kalibrasyon metrikleri güçlüdür ve modelin gerçek kredi tahsis süreçlerinde kullanılabilir olduğunu göstermektedir.<br/><br/>\n",
        "\n",
        "Model; kredi fiyatlama, limit belirleme, risk sınıflandırması, erken uyarı sistemleri ve müşteri segmentasyonu\n",
        "gibi iş alanlarında güvenle uygulanabilir niteliktedir.\n",
        "\n",
        "Model aynı zamanda SHAP analizi sayesinde güçlü bir açıklanabilirlik sunmakta,\n",
        "bu da hem düzenleyici gereklilikler hem de iş birimleri için önemli bir avantajdır.\n",
        "\"\"\")\n",
        "\n",
        "# =========================================================\n",
        "# 11) GENERATE PDF\n",
        "# =========================================================\n",
        "\n",
        "doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "doc.build(story)\n",
        "\n",
        "print(f\"\\n📄 FINAL REPORT CREATED SUCCESSFULLY:\\n{pdf_path}\")\n"
      ],
      "metadata": {
        "id": "68JCBuiF0-En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# BANKA TİPİ MODEL VALIDATION PDF RAPORU (REPORTLAB)\n",
        "# =============================================================\n",
        "\n",
        "!pip install reportlab\n",
        "\n",
        "\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib import colors\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle\n",
        "from reportlab.lib.units import cm\n",
        "import pandas as pd\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# =============================================================\n",
        "# LOAD METRICS & WEIGHTS\n",
        "# =============================================================\n",
        "\n",
        "# Load OOF predictions\n",
        "df = pd.read_csv(f\"{BASE}/oof_predictions_hack.csv\")\n",
        "\n",
        "# Load best weights\n",
        "with open(f\"{BASE}/best_blending_weights.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    best_line = [l for l in lines if \"Weights:\" in l][0]\n",
        "    parts = best_line.replace(\"Weights:\", \"\").strip().split(\",\")\n",
        "    w_cat = float(parts[0].split(\"=\")[1])\n",
        "    w_lgb = float(parts[1].split(\"=\")[1])\n",
        "    w_xgb = float(parts[2].split(\"=\")[1])\n",
        "\n",
        "# Compute blend AUC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "blend = w_cat*df[\"oof_cat\"] + w_lgb*df[\"oof_lgb\"] + w_xgb*df[\"oof_xgb\"]\n",
        "auc_blend = roc_auc_score(df[\"TARGET\"], blend)\n",
        "\n",
        "# Load KS score\n",
        "with open(f\"{BASE}/ks_score.txt\", \"r\") as f:\n",
        "    ks_value = f.read().replace(\"KS Score =\",\"\").strip()\n",
        "\n",
        "# =============================================================\n",
        "# CREATE PDF DOCUMENT\n",
        "# =============================================================\n",
        "pdf_path = f\"{BASE}/Model_Validation_Report.pdf\"\n",
        "doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "\n",
        "styles = getSampleStyleSheet()\n",
        "title_style = styles['Title']\n",
        "normal = styles['BodyText']\n",
        "subtitle = ParagraphStyle(\n",
        "    'Subtitle',\n",
        "    parent=styles['Heading2'],\n",
        "    textColor=colors.darkblue\n",
        ")\n",
        "\n",
        "story = []\n",
        "\n",
        "# =============================================================\n",
        "# TITLE PAGE\n",
        "# =============================================================\n",
        "\n",
        "story.append(Paragraph(\"🏦 Credit Risk Model Validation Report\", title_style))\n",
        "story.append(Spacer(1, 12))\n",
        "\n",
        "story.append(Paragraph(\"Home Credit Default Risk — Ensemble Model\", subtitle))\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "story.append(Paragraph(f\"<b>Best Blending Weights</b>:  CAT={w_cat},  LGB={w_lgb},  XGB={w_xgb}\", normal))\n",
        "story.append(Paragraph(f\"<b>Blending AUC (OOF Approx)</b>:  {auc_blend:.5f}\", normal))\n",
        "story.append(Paragraph(f\"<b>KS Score</b>:  {ks_value}\", normal))\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "# =============================================================\n",
        "# METRIC TABLE\n",
        "# =============================================================\n",
        "\n",
        "table_data = [\n",
        "    [\"Metric\", \"Value\"],\n",
        "    [\"AUC (OOF Blended)\", f\"{auc_blend:.5f}\"],\n",
        "    [\"KS Score\", ks_value],\n",
        "    [\"Cat Weight\", w_cat],\n",
        "    [\"LGB Weight\", w_lgb],\n",
        "    [\"XGB Weight\", w_xgb]\n",
        "]\n",
        "\n",
        "table = Table(table_data, colWidths=[6*cm, 6*cm])\n",
        "table.setStyle(TableStyle([\n",
        "    ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),\n",
        "    ('TEXTCOLOR',(0,0),(-1,0),colors.black),\n",
        "    ('ALIGN',(0,0),(-1,-1),'CENTER'),\n",
        "    ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n",
        "    ('FONTSIZE', (0,0), (-1,-1), 11),\n",
        "    ('BOTTOMPADDING', (0,0), (-1,0), 12),\n",
        "    ('GRID', (0,0), (-1,-1), 0.5, colors.grey)\n",
        "]))\n",
        "story.append(table)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "# =============================================================\n",
        "# INSERT ROC CURVE\n",
        "# =============================================================\n",
        "\n",
        "story.append(Paragraph(\"ROC Curve\", subtitle))\n",
        "roc_img = Image(f\"{BASE}/roc_curve_blended.png\", width=14*cm, height=10*cm)\n",
        "story.append(roc_img)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "# =============================================================\n",
        "# INSERT PRECISION-RECALL\n",
        "# =============================================================\n",
        "\n",
        "story.append(Paragraph(\"Precision-Recall Curve\", subtitle))\n",
        "pr_img = Image(f\"{BASE}/pr_curve_blended.png\", width=14*cm, height=10*cm)\n",
        "story.append(pr_img)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "# =============================================================\n",
        "# INSERT CALIBRATION\n",
        "# =============================================================\n",
        "\n",
        "story.append(Paragraph(\"Calibration Plot\", subtitle))\n",
        "cal_img = Image(f\"{BASE}/calibration_curve_blended.png\", width=14*cm, height=10*cm)\n",
        "story.append(cal_img)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# BUILD PDF\n",
        "# =============================================================\n",
        "\n",
        "doc.build(story)\n",
        "\n",
        "print(\"PDF Report saved to:\", pdf_path)\n"
      ],
      "metadata": {
        "id": "oj2ShjlISw59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CHUNK 6 — CORRECT SHAP ANALYSIS (CATBOOST NATIVE SHAP)\n",
        "# ==========================================================\n",
        "\n",
        "!pip install catboost shap\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# LOAD TRAIN DATA\n",
        "# ----------------------------------------------------------\n",
        "train = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "X = train.drop(columns=[\"TARGET\"])\n",
        "print(\"Train shape:\", X.shape)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# LOAD FINAL CATBOOST MODEL (fold5)\n",
        "# ----------------------------------------------------------\n",
        "cat = CatBoostClassifier()\n",
        "cat.load_model(f\"{BASE}/catboost_model_fold5.cbm\")\n",
        "print(\"CatBoost model loaded.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SAMPLE FOR SHAP (10k rows, RAM-safe)\n",
        "# ----------------------------------------------------------\n",
        "sample_idx = np.random.choice(len(X), 10000, replace=False)\n",
        "X_sample = X.iloc[sample_idx]\n",
        "pool_sample = Pool(X_sample)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# CATBOOST OFFICIAL SHAP VALUES (CORRECT METHOD)\n",
        "# ----------------------------------------------------------\n",
        "print(\"Computing correct SHAP (CatBoost native)...\")\n",
        "\n",
        "# CatBoost returns matrix: (n_samples, n_features + 1)\n",
        "# Last column = expected_value → drop it\n",
        "shap_raw = cat.get_feature_importance(\n",
        "    data=pool_sample,\n",
        "    type=\"ShapValues\"\n",
        ")\n",
        "\n",
        "print(\"Raw SHAP shape:\", shap_raw.shape)\n",
        "\n",
        "# Remove the last column (bias term)\n",
        "shap_values = shap_raw[:, :-1]\n",
        "\n",
        "print(\"Correct SHAP matrix shape:\", shap_values.shape)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SHAP SUMMARY PLOT (DISPLAY + SAVE)\n",
        "# ----------------------------------------------------------\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_sample,\n",
        "    show=False\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{BASE}/shap_summary.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "print(\"Saved: shap_summary.png\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SHAP BAR PLOT (DISPLAY + SAVE)\n",
        "# ----------------------------------------------------------\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_sample,\n",
        "    plot_type=\"bar\",\n",
        "    show=False\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{BASE}/shap_bar.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "print(\"Saved: shap_bar.png\")\n",
        "\n",
        "print(\"\\nCHUNK 6 — SHAP ANALYSIS COMPLETED (CORRECT VERSION).\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Yg0owKLsUIzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CHUNK 7 — EXTENDED SHAP ANALYSIS (CATBOOST OFFICIAL SHAP)\n",
        "# ==========================================================\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import os\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# LOAD TRAIN DATA\n",
        "# ----------------------------------------------------------\n",
        "train = pd.read_parquet(f\"{BASE}/final_train_reduced.parquet\")\n",
        "X = train.drop(columns=[\"TARGET\"])\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# LOAD CATBOOST MODEL\n",
        "# ----------------------------------------------------------\n",
        "cat = CatBoostClassifier()\n",
        "cat.load_model(f\"{BASE}/catboost_model_fold5.cbm\")\n",
        "print(\"CatBoost model loaded.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SAMPLE FOR SHAP (5000 is enough for extended analysis)\n",
        "# ----------------------------------------------------------\n",
        "sample_idx = np.random.choice(len(X), 5000, replace=False)\n",
        "X_sample = X.iloc[sample_idx]\n",
        "pool_sample = Pool(X_sample)\n",
        "\n",
        "print(\"Computing SHAP values (CatBoost native)...\")\n",
        "\n",
        "# CatBoost SHAP: shape = (n_samples, n_features + 1)\n",
        "shap_raw = cat.get_feature_importance(\n",
        "    data=pool_sample,\n",
        "    type=\"ShapValues\"\n",
        ")\n",
        "\n",
        "print(\"Raw SHAP:\", shap_raw.shape)\n",
        "\n",
        "# Last column = expected_value → drop it\n",
        "shap_values = shap_raw[:, :-1]\n",
        "\n",
        "print(\"Correct SHAP:\", shap_values.shape)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1) SAVE GLOBAL FEATURE IMPORTANCE (CSV)\n",
        "# ----------------------------------------------------------\n",
        "importance = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    \"feature\": X_sample.columns,\n",
        "    \"shap_importance\": importance\n",
        "}).sort_values(\"shap_importance\", ascending=False)\n",
        "\n",
        "importance_path = f\"{BASE}/shap_feature_importance.csv\"\n",
        "importance_df.to_csv(importance_path, index=False)\n",
        "\n",
        "print(\"Saved:\", importance_path)\n",
        "\n",
        "# Select top 10 features\n",
        "top10 = importance_df.head(10)[\"feature\"].tolist()\n",
        "print(\"Top 10 features:\", top10)\n",
        "\n",
        "# Make directory\n",
        "os.makedirs(f\"{BASE}/shap_plots\", exist_ok=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2) DEPENDENCE PLOTS (TOP 10)\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\nGenerating SHAP dependence plots...\")\n",
        "\n",
        "for feat in top10:\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    shap.dependence_plot(\n",
        "        feat,\n",
        "        shap_values,\n",
        "        X_sample,\n",
        "        show=False\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{BASE}/shap_plots/shap_dependence_{feat}.png\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"Dependence plots saved → shap_plots/*.png\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3) DECISION PLOT (300 samples for clarity)\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\nGenerating SHAP decision plot...\")\n",
        "\n",
        "shap_subset = shap_values[:300]\n",
        "X_subset = X_sample.iloc[:300]\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "shap.decision_plot(\n",
        "    base_value=shap_raw[0, -1],   # CatBoost expected value\n",
        "    shap_values=shap_subset,\n",
        "    feature_names=list(X_subset.columns),\n",
        "    show=False\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{BASE}/shap_decision_plot.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved shap_decision_plot.png\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# FINISHED\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\nCHUNK 7 — EXTENDED SHAP ANALYSIS COMPLETED SUCCESSFULLY.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CthELlkLVCm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tek müsteri icin waterfall"
      ],
      "metadata": {
        "id": "VdWAxotFaGis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CLASSIC WATERFALL PLOT FOR A SINGLE CUSTOMER\n",
        "# ==========================================================\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1 müşteri seçelim\n",
        "index = 0   # istediğini seçebilirsin\n",
        "x_single = X_sample.iloc[index:index+1]\n",
        "\n",
        "# SHAP değerini çek\n",
        "shap_single = shap_values[index]\n",
        "\n",
        "# Waterfall çiz\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.waterfall_plot(\n",
        "    shap.Explanation(\n",
        "        values = shap_single,\n",
        "        base_values = shap_raw[0, -1],\n",
        "        data = x_single.values[0],\n",
        "        feature_names = list(X_sample.columns)\n",
        "    )\n",
        ")\n",
        "plt.savefig(f\"{BASE}/shap_waterfall_single.png\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved: shap_waterfall_single.png\")\n"
      ],
      "metadata": {
        "id": "-11RC16waIk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/HomeCredit/Aggregated_tables/\n",
        "\n"
      ],
      "metadata": {
        "id": "61V_E5C9csU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# FINAL MODEL VALIDATION PDF REPORT (FULL & FIXED VERSION)\n",
        "# ================================================================\n",
        "\n",
        "!pip install reportlab\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.units import cm\n",
        "from reportlab.lib.utils import ImageReader\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/HomeCredit/Aggregated_tables\"\n",
        "\n",
        "# ================================================================\n",
        "# SAFE IMAGE LOADER (Fixes ReportLab rendering issues)\n",
        "# ================================================================\n",
        "def safe_image(path, max_width=14*cm, max_height=10*cm):\n",
        "    \"\"\"\n",
        "    Loads an image safely and rescales it so it always fits into PDF.\n",
        "    Prevents ReportLab missing-image bugs.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"❌ Missing:\", path)\n",
        "        return None\n",
        "\n",
        "    img = ImageReader(path)\n",
        "    iw, ih = img.getSize()\n",
        "\n",
        "    scale = min(max_width / iw, max_height / ih)\n",
        "    return Image(path, width=iw * scale, height=ih * scale)\n",
        "\n",
        "# ================================================================\n",
        "# LOAD METRICS\n",
        "# ================================================================\n",
        "blend = pd.read_csv(f\"{BASE}/blending_weight_results.csv\")\n",
        "best_row = blend.loc[blend[\"auc\"].idxmax()]\n",
        "best_auc = best_row[\"auc\"]\n",
        "w_cat, w_lgb, w_xgb = best_row[\"cat\"], best_row[\"lgb\"], best_row[\"xgb\"]\n",
        "\n",
        "with open(f\"{BASE}/ks_score.txt\",\"r\") as f:\n",
        "    ks_score = f.read().replace(\"KS Score =\",\"\").strip()\n",
        "\n",
        "shap_fi = pd.read_csv(f\"{BASE}/shap_feature_importance.csv\").head(15)\n",
        "\n",
        "print(\"Loaded metrics successfully.\")\n",
        "\n",
        "# ================================================================\n",
        "# BUILD PDF\n",
        "# ================================================================\n",
        "pdf_path = f\"{BASE}/Model_Report_Final.pdf\"\n",
        "doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "styles = getSampleStyleSheet()\n",
        "title = styles[\"Title\"]\n",
        "subtitle = styles[\"Heading2\"]\n",
        "normal = styles[\"BodyText\"]\n",
        "\n",
        "story = []\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# TITLE SECTION\n",
        "# ---------------------------------------------------------------\n",
        "story.append(Paragraph(\"Home Credit Default Risk — Model Validation Report\", title))\n",
        "story.append(Spacer(1, 12))\n",
        "story.append(Paragraph(\"Prepared for Data Science Tutor — Full Project Summary\", subtitle))\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "story.append(Paragraph(f\"<b>Final Blended AUC</b>: {best_auc:.5f}\", normal))\n",
        "story.append(Paragraph(f\"<b>KS Score</b>: {ks_score}\", normal))\n",
        "story.append(Paragraph(f\"<b>Blending Weights</b>: Cat={w_cat}, LGB={w_lgb}, XGB={w_xgb}\", normal))\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# FEATURE IMPORTANCE TABLE\n",
        "# ---------------------------------------------------------------\n",
        "story.append(Paragraph(\"Top 15 SHAP Features\", subtitle))\n",
        "\n",
        "table_data = [[\"Feature\", \"Importance\"]]\n",
        "for _, row in shap_fi.iterrows():\n",
        "    table_data.append([row[\"feature\"], f\"{row['shap_importance']:.6f}\"])\n",
        "\n",
        "table = Table(table_data, colWidths=[8*cm, 5*cm])\n",
        "table.setStyle(TableStyle([\n",
        "    (\"BACKGROUND\", (0,0), (-1,0), colors.lightgrey),\n",
        "    (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n",
        "    (\"ALIGN\", (0,0), (-1,-1), \"CENTER\"),\n",
        "    (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey)\n",
        "]))\n",
        "\n",
        "story.append(table)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# METRIC PLOTS (ROC / PR / CALIBRATION)\n",
        "# ---------------------------------------------------------------\n",
        "story.append(Paragraph(\"Model Performance Metrics\", subtitle))\n",
        "\n",
        "metric_plots = [\n",
        "    (\"ROC Curve\", \"roc_curve_blended.png\"),\n",
        "    (\"Precision-Recall Curve\", \"pr_curve_blended.png\"),\n",
        "    (\"Calibration Curve\", \"calibration_curve_blended.png\"),\n",
        "]\n",
        "\n",
        "for header, filename in metric_plots:\n",
        "    path = f\"{BASE}/{filename}\"\n",
        "    story.append(Paragraph(header, subtitle))\n",
        "    img = safe_image(path)\n",
        "    if img:\n",
        "        story.append(img)\n",
        "    else:\n",
        "        story.append(Paragraph(f\"Missing image: {filename}\", normal))\n",
        "    story.append(Spacer(1, 24))\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# GLOBAL SHAP PLOTS\n",
        "# ---------------------------------------------------------------\n",
        "story.append(Paragraph(\"SHAP Summary Plot\", subtitle))\n",
        "img = safe_image(f\"{BASE}/shap_summary.png\")\n",
        "if img: story.append(img)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "story.append(Paragraph(\"SHAP Bar Plot\", subtitle))\n",
        "img = safe_image(f\"{BASE}/shap_bar.png\")\n",
        "if img: story.append(img)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# SHAP DEPENDENCE PLOTS\n",
        "# ---------------------------------------------------------------\n",
        "story.append(Paragraph(\"SHAP Dependence Plots\", subtitle))\n",
        "\n",
        "dep_dir = f\"{BASE}/shap_plots\"\n",
        "dep_files = sorted(os.listdir(dep_dir))\n",
        "\n",
        "for f in dep_files:\n",
        "    story.append(Paragraph(f.replace(\".png\", \"\"), normal))\n",
        "    img = safe_image(os.path.join(dep_dir, f))\n",
        "    if img: story.append(img)\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# SHAP DECISION & WATERFALL\n",
        "# ---------------------------------------------------------------\n",
        "story.append(Paragraph(\"SHAP Decision Plot\", subtitle))\n",
        "img = safe_image(f\"{BASE}/shap_decision_plot.png\")\n",
        "if img: story.append(img)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "story.append(Paragraph(\"SHAP Waterfall (Single Prediction)\", subtitle))\n",
        "img = safe_image(f\"{BASE}/shap_waterfall_single.png\")\n",
        "if img: story.append(img)\n",
        "story.append(Spacer(1, 24))\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# BUILD PDF\n",
        "# ---------------------------------------------------------------\n",
        "doc.build(story)\n",
        "print(\"\\n🎉 PDF created successfully:\")\n",
        "print(pdf_path)\n"
      ],
      "metadata": {
        "id": "NRapxb03ghPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "import shutil\n",
        "\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/aggregated_master_tables.ipynb\"\n",
        "\n",
        "# Yedek oluştur\n",
        "shutil.copy(file, file + \".backup\")\n",
        "\n",
        "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Metadata temizliği\n",
        "nb.metadata.pop(\"widgets\", None)\n",
        "for cell in nb.cells:\n",
        "    cell.metadata.pop(\"widgets\", None)\n",
        "\n",
        "with open(file, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(\"Metadata temizliği tamamlandı! Artık bu notebook GitHub'da sorunsuz açılacak.\")\n",
        "print(\"Yedek dosyan: aggregated_master_tables.ipynb.backup\")\n"
      ],
      "metadata": {
        "id": "Uf_IHVIjDzqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}